{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kv772/Accenture1D_AIStudio/blob/main/Accenture_1D_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58gsOM4OTIcx"
      },
      "source": [
        "# Accenture 1D: Content Moderation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtfe_HialYVg"
      },
      "outputs": [],
      "source": [
        "# Mandatory Installs\n",
        "# Used for language analysis\n",
        "%pip install langdetect\n",
        "%pip install wordfreq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnlmyz9pTC6n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from collections import Counter\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "from multiprocessing import Pool\n",
        "from wordfreq import zipf_frequency\n",
        "from multiprocessing import Pool\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import sys, subprocess\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGUV1oxQXoli"
      },
      "source": [
        "# File Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ECrPb_gTb29"
      },
      "outputs": [],
      "source": [
        "# Connect Google Colab and Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbLu2tLnUHQB"
      },
      "outputs": [],
      "source": [
        "datasets = \"/content/drive/MyDrive/BTT Accenture 1D!/Fake News Detection Datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-weODU8US4Q"
      },
      "outputs": [],
      "source": [
        "true_df = pd.read_csv(f\"{datasets}/True.csv\")\n",
        "fake_df = pd.read_csv(f\"{datasets}/Fake.csv\")\n",
        "print(true_df.shape)\n",
        "print(fake_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq16mCnWXxAQ"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIC6gGv-NG9A"
      },
      "source": [
        "## Location Analysis (Adriena Jiang)\n",
        "\n",
        "**What is this about:** This analysis is checking the number of articles (in both datasets) that contain a location (specifically a city or a country).\n",
        "\n",
        "**What I found:** 19588/21417 true articles contain a location and 18862/23481 fake articles contain a location.\n",
        "\n",
        "**Why this matters:** Initially as a team we thought that location might have caused our model to overfit as it seemed location was always attached to Reuter mentions. Turns out most articles in both data sets contain location.\n",
        "\n",
        "**What I suggest**: I suggest we keep locations. Location mentions are throughout both data sets so there is no need to clean them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2b0i0R7NLK-"
      },
      "outputs": [],
      "source": [
        "# Analyzing how many articles have location attached\n",
        "\n",
        "# GeoText has built in list of cities and countries\n",
        "!pip install geotext\n",
        "from geotext import GeoText\n",
        "\n",
        "# Function to check if an article contains a location\n",
        "# Returns true if the article text contains a city or country name\n",
        "# Returns false otherwise\n",
        "def contains_location(text):\n",
        "    # Check that text is a string and not empty\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return False\n",
        "    # Creates a geotext object for the article body\n",
        "    places = GeoText(text)\n",
        "    # Check if it contains cities or countries\n",
        "    return bool(places.cities or places.countries)\n",
        "\n",
        "# Add a boolean column for each dataset\n",
        "true_df[\"has_location\"] = true_df[\"text\"].astype(str).apply(contains_location)\n",
        "fake_df[\"has_location\"] = fake_df[\"text\"].astype(str).apply(contains_location)\n",
        "\n",
        "# Statistics\n",
        "for name, df in [(\"True\", true_df), (\"Fake\", fake_df)]:\n",
        "    total = len(df)\n",
        "    loc_count = df[\"has_location\"].sum()\n",
        "    print(f\"{name} articles with a detected location: \" f\"{loc_count} / {total} ({loc_count/total:.1%})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxJbDh0vP0-l"
      },
      "outputs": [],
      "source": [
        "# Look at the new column in the true dataset\n",
        "true_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZe0TaDbQG-L"
      },
      "outputs": [],
      "source": [
        "# Look at the new column in the fake dataset\n",
        "fake_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFHvHf5dQZj"
      },
      "source": [
        "## Punctuation, Misspelling, and Emoji Analysis (Ousman)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_5ZAKh_cUH9"
      },
      "outputs": [],
      "source": [
        "!pip install emoji pyspellchecker\n",
        "import re\n",
        "import emoji\n",
        "from collections import Counter\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "def count_punctuation(text):\n",
        "    return len(re.findall(r'[^\\w\\s]', str(text)))\n",
        "\n",
        "def extract_emojis(text):\n",
        "    return [c for c in str(text) if c in emoji.EMOJI_DATA]\n",
        "\n",
        "def count_misspellings(text):\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', str(text).lower())\n",
        "    misspelled = spell.unknown(words)\n",
        "    return len(misspelled)\n",
        "\n",
        "def run_analysis(df, title=\"\"):\n",
        "    print(f\"\\n=========== {title} ===========\")\n",
        "\n",
        "    analysis = pd.DataFrame()\n",
        "    analysis[\"text\"] = df[\"text\"]\n",
        "\n",
        "    # Compute metrics\n",
        "    analysis[\"punctuation_count\"] = analysis[\"text\"].apply(count_punctuation)\n",
        "    analysis[\"emoji_count\"]       = analysis[\"text\"].apply(lambda x: len(extract_emojis(x)))\n",
        "    analysis[\"misspelling_count\"] = analysis[\"text\"].apply(count_misspellings)\n",
        "\n",
        "    # Summary outputs\n",
        "    print(\"\\nPunctuation Statistics:\")\n",
        "    print(analysis[\"punctuation_count\"].describe())\n",
        "\n",
        "    print(\"\\nEmoji Statistics:\")\n",
        "    print(analysis[\"emoji_count\"].describe())\n",
        "\n",
        "    print(\"\\nMisspelling Statistics:\")\n",
        "    print(analysis[\"misspelling_count\"].describe())\n",
        "\n",
        "    # Emoji frequency\n",
        "    all_emojis = []\n",
        "    analysis[\"text\"].apply(lambda x: all_emojis.extend(extract_emojis(x)))\n",
        "    emoji_counts = Counter(all_emojis)\n",
        "\n",
        "    print(\"\\nMost Common Emojis:\")\n",
        "    print(emoji_counts.most_common(10))\n",
        "\n",
        "    return analysis, emoji_counts\n",
        "true_analysis, true_emoji_counts = run_analysis(true_df, \"TRUE NEWS ANALYSIS\")\n",
        "fake_analysis, fake_emoji_counts = run_analysis(fake_df, \"FAKE NEWS ANALYSIS\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkuZahyWdjZ-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Emoji\n",
        "true_emoji_df = pd.DataFrame(true_emoji_counts.most_common(15), columns=[\"emoji\", \"true_freq\"])\n",
        "fake_emoji_df = pd.DataFrame(fake_emoji_counts.most_common(15), columns=[\"emoji\", \"fake_freq\"])\n",
        "\n",
        "\n",
        "emoji_compare = pd.merge(true_emoji_df, fake_emoji_df, on=\"emoji\", how=\"outer\").fillna(0)\n",
        "\n",
        "\n",
        "emoji_melted = emoji_compare.melt(id_vars=\"emoji\",\n",
        "                                  value_vars=[\"true_freq\",\"fake_freq\"],\n",
        "                                  var_name=\"dataset\", value_name=\"frequency\")\n",
        "\n",
        "if emoji_melted.empty:\n",
        "    print(\"No emojis found in either dataset.\")\n",
        "else:\n",
        "    sns.barplot(data=emoji_melted, x=\"frequency\", y=\"emoji\", hue=\"dataset\")\n",
        "    plt.title(\"Emoji Frequency: True vs Fake\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4fOkDxVm8_D"
      },
      "outputs": [],
      "source": [
        "# Average misspellings bar chart\n",
        "true_avg = true_analysis[\"misspelling_count\"].mean()\n",
        "fake_avg = fake_analysis[\"misspelling_count\"].mean()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar([\"True News\", \"Fake News\"], [true_avg, fake_avg], color=[\"blue\", \"orange\"])\n",
        "plt.title(\"Average Misspellings: True vs Fake\")\n",
        "plt.ylabel(\"Average Misspelling Count\")\n",
        "plt.show()\n",
        "\n",
        "print(\"True Avg Misspellings:\", true_avg)\n",
        "print(\"Fake Avg Misspellings:\", fake_avg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvP3E1abm8wA"
      },
      "outputs": [],
      "source": [
        "#punctuation\n",
        "true_punc_avg = true_analysis[\"punctuation_count\"].mean()\n",
        "fake_punc_avg = fake_analysis[\"punctuation_count\"].mean()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar([\"True News\", \"Fake News\"], [true_punc_avg, fake_punc_avg], color=[\"blue\", \"orange\"])\n",
        "plt.title(\"Average Punctuation Count: True vs Fake\")\n",
        "plt.ylabel(\"Average Punctuation Count\")\n",
        "plt.show()\n",
        "\n",
        "print(\"True Avg Punctuation:\", true_punc_avg)\n",
        "print(\"Fake Avg Punctuation:\", fake_punc_avg)\n",
        "\n",
        "\n",
        "\n",
        "print(\"TRUE Punctuation Count\")\n",
        "display(true_analysis.sort_values(\"punctuation_count\", ascending=False).head(10))\n",
        "\n",
        "print(\"FAKE Punctuation Count\")\n",
        "display(fake_analysis.sort_values(\"punctuation_count\", ascending=False).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp03tsL2haMK"
      },
      "source": [
        "## Inspecting the Data Set (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkSEKhGKUYuI"
      },
      "outputs": [],
      "source": [
        "# Inspecting shape and columns for true data set\n",
        "print(\"True shape:\", true_df.shape)\n",
        "print(\"True Columns: \", true_df.columns)\n",
        "# Inspecting the shape and column for fake data set\n",
        "print(\"Fake shape:\", fake_df.shape)\n",
        "print(\"Fake Columns: \", fake_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8i9IPcYYBOh"
      },
      "outputs": [],
      "source": [
        "# Take a look at the true data set\n",
        "true_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te_aKJ34YBU-"
      },
      "outputs": [],
      "source": [
        "# Take a look at the fake data set\n",
        "fake_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5NTcPa-Wd5S"
      },
      "source": [
        "## Quality Issue Analysis (Harshika)\n",
        "\n",
        "**What I found:** Fake news articles are noisier, there have blanks (2.7%), short texts (1.0%), and many special character spikes. Sentence lengths also vary more, with many outliers in Fake news.\n",
        "\n",
        "**Why this matters:** The model can learn to classify based on noise or structure (e.g. formatting, length), not actual content which leads to overfitting or data leakage.\n",
        "\n",
        "**What I suggest:** Remove blank/short texts, cap extreme special chars, and normalize formatting to ensure content-focused learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vylfDi_u7mV1"
      },
      "outputs": [],
      "source": [
        "#Harshika Text quality issue analysis\n",
        "import re\n",
        "\n",
        "#count special chars like @, #, !\n",
        "def count_chars(text):\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return 0\n",
        "    return len(re.findall(r'[^a-zA-Z0-9\\s]', text))\n",
        "\n",
        "#count sentences split by .\n",
        "def count_sents(text):\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return 0\n",
        "    sents = [s for s in text.split('.') if s.strip()]\n",
        "    return len(sents)\n",
        "\n",
        "#avg words per sentence\n",
        "def sent_len(text):\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return 0\n",
        "    words = len(text.split())\n",
        "    sents = count_sents(text)\n",
        "    return words / sents if sents > 0 else 0\n",
        "\n",
        "#word count\n",
        "def word_cnt(text):\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return 0\n",
        "    return len(text.split())\n",
        "\n",
        "#add columns to true data\n",
        "true_df['chars'] = true_df['text'].apply(count_chars)\n",
        "true_df['sent_len'] = true_df['text'].apply(sent_len)\n",
        "true_df['words'] = true_df['text'].apply(word_cnt)\n",
        "true_df['blanks'] = true_df['text'].apply(lambda x: pd.isna(x) or x.strip() == \"\")\n",
        "true_df['short'] = true_df['words'].apply(lambda x: x < 10 and x > 0)\n",
        "\n",
        "#add columns to fake data\n",
        "fake_df['chars'] = fake_df['text'].apply(count_chars)\n",
        "fake_df['sent_len'] = fake_df['text'].apply(sent_len)\n",
        "fake_df['words'] = fake_df['text'].apply(word_cnt)\n",
        "fake_df['blanks'] = fake_df['text'].apply(lambda x: pd.isna(x) or x.strip() == \"\")\n",
        "fake_df['short'] = fake_df['words'].apply(lambda x: x < 10 and x > 0)\n",
        "\n",
        "#count missing NaN\n",
        "true_nan_text = true_df['text'].isna().sum()\n",
        "true_nan_title = true_df['title'].isna().sum()\n",
        "fake_nan_text = fake_df['text'].isna().sum()\n",
        "fake_nan_title = fake_df['title'].isna().sum()\n",
        "\n",
        "print(\"\\n True News Quality ---\")\n",
        "print(f\"Missing Text: {true_nan_text} ({100 * true_nan_text / len(true_df):.1f}%)\")\n",
        "print(f\"Missing Title: {true_nan_title} ({100 * true_nan_title / len(true_df):.1f}%)\")\n",
        "print(f\"Blank Text: {true_df['blanks'].sum()} ({100 * true_df['blanks'].sum() / len(true_df):.1f}%)\")\n",
        "print(f\"Short Text (<10 words): {true_df['short'].sum()} ({100 * true_df['short'].sum() / len(true_df):.1f}%)\")\n",
        "print(f\"Avg Special Chars: {true_df['chars'].mean():.1f}\")\n",
        "print(f\"Avg Sentence Length: {true_df['sent_len'].mean():.1f} words\")\n",
        "\n",
        "print(\"\\n Fake News Quality ---\")\n",
        "print(f\"Missing Text: {fake_nan_text} ({100 * fake_nan_text / len(fake_df):.1f}%)\")\n",
        "print(f\"Missing Title: {fake_nan_title} ({100 * fake_nan_title / len(fake_df):.1f}%)\")\n",
        "print(f\"Blank Text: {fake_df['blanks'].sum()} ({100 * fake_df['blanks'].sum() / len(fake_df):.1f}%)\")\n",
        "print(f\"Short Text (<10 words): {fake_df['short'].sum()} ({100 * fake_df['short'].sum() / len(fake_df):.1f}%)\")\n",
        "print(f\"Avg Special Chars: {fake_df['chars'].mean():.1f}\")\n",
        "print(f\"Avg Sentence Length: {fake_df['sent_len'].mean():.1f} words\")\n",
        "\n",
        "#problem articles\n",
        "print(\"\\n True: Top 5 Noisy Articles ---\")\n",
        "print(true_df.nlargest(5, 'chars')[['text', 'chars', 'blanks', 'short']])\n",
        "\n",
        "print(\"\\n Fake: Top 5 Noisy Articles ---\")\n",
        "print(fake_df.nlargest(5, 'chars')[['text', 'chars', 'blanks', 'short']])\n",
        "\n",
        "#special chars plot\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(true_df['chars'], bins=15, color='blue', label='True', alpha=0.5)\n",
        "sns.histplot(fake_df['chars'], bins=15, color='red', label='Fake', alpha=0.5)\n",
        "plt.title(\"Special Characters in Text\")\n",
        "plt.xlabel(\"Special Chars\")\n",
        "plt.ylabel(\"Articles\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#sentence length plot\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(data=[true_df['sent_len'], fake_df['sent_len']], palette=['blue', 'red'])\n",
        "plt.xticks([0,1], ['True', 'Fake'])\n",
        "plt.title(\"Words per Sentence\")\n",
        "plt.ylabel(\"Words\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpua4HbnWrH9"
      },
      "source": [
        "## Word Length Analysis (Ousman Bah)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxHzx_1EQaC-"
      },
      "outputs": [],
      "source": [
        "fake_df['word_count'] = fake_df['text'].str.split().str.len()\n",
        "fake_df['char_count'] = fake_df['text'].str.len()\n",
        "true_df['word_count'] = true_df['text'].str.split().str.len()\n",
        "true_df['char_count'] = true_df['text'].str.len()\n",
        "print(\"FAKE :\",fake_df[['word_count', 'char_count']].describe())\n",
        "print(  )\n",
        "print(\"TRUE :\",true_df[['word_count', 'char_count']].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GucKnaBjRi_"
      },
      "source": [
        "## Blank Text (Ousman Bah)\n",
        "\n",
        "**What I found:**\n",
        "Only one article from the True News dataset was blank, while about 630 articles from the Fake News dataset contained no text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxwAhURIWc0Y"
      },
      "outputs": [],
      "source": [
        "# --- OUSMAN BAH: FULL BLANK ARTICLES PRINT & VISUALIZATION ---\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming your datasets already exist: fake_df and true_df\n",
        "\n",
        "# --- Count Blank vs Non-Blank Articles ---\n",
        "blank_fake = fake_df['text'].str.strip().eq(\"\").sum()\n",
        "non_blank_fake = len(fake_df) - blank_fake\n",
        "\n",
        "blank_true = true_df['text'].str.strip().eq(\"\").sum()\n",
        "non_blank_true = len(true_df) - blank_true\n",
        "\n",
        "# --- Print Counts ---\n",
        "print(\"\\nBlank Fake Articles:\", blank_fake)\n",
        "print(\"Non-Blank Fake Articles:\", non_blank_fake)\n",
        "print(\"Blank True Articles:\", blank_true)\n",
        "print(\"Non-Blank True Articles:\", non_blank_true)\n",
        "\n",
        "# --- Visualization ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(\n",
        "    x=[\"Fake Blank\", \"Fake Non-Blank\", \"True Blank\", \"True Non-Blank\"],\n",
        "    y=[blank_fake, non_blank_fake, blank_true, non_blank_true],\n",
        "    palette=[\"red\", \"red\", \"blue\", \"blue\"]\n",
        ")\n",
        "plt.title(\"Blank vs Non-Blank Articles in Fake and True Datasets\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tTWzZ-qkU96"
      },
      "source": [
        "## Articles Ending with '...' (Ousman Bah)\n",
        "\n",
        "What I found: Only two articles from the True Dataset ended with \"....\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKrDaMRjR6nu"
      },
      "outputs": [],
      "source": [
        "fake_trunc = fake_df[fake_df['text'].str.contains(r\"\\.\\.\\.\\s*$\", na=False)]\n",
        "true_trunc = true_df[true_df['text'].str.contains(r\"\\.\\.\\.\\s*$\", na=False)]\n",
        "\n",
        "print(\"Fake Articles ending with ellipses:\", len(fake_trunc))\n",
        "print()\n",
        "print(\"--------------------------------\")\n",
        "print()\n",
        "print(\"True Articles ending with ellipses:\", len(true_trunc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbJytwMOkbq-"
      },
      "source": [
        "## Articles Length Visualization: Line Graph (Ousman Bah)\n",
        "**WHAT I FOUND:**\n",
        "\n",
        "The plot compares the word counts of all articles in both datasets:\n",
        "\n",
        "**The red line** (Fake Articles) has more sharp spikes, showing high variation in article length — some are extremely long.\n",
        "\n",
        "**The blue line** (True Articles) is denser and flatter, indicating more uniform article lengths.\n",
        "\n",
        "**Overall, the Fake dataset shows greater inconsistency and outliers, while the True dataset maintains a more stable pattern.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipICHcQeVVwM"
      },
      "outputs": [],
      "source": [
        "print(\"Long Fake Articles (>2000 words):\",\n",
        "      len(fake_df[fake_df['word_count'] > 2000]))\n",
        "\n",
        "print(fake_df[fake_df['word_count'] > 2000]['text'].head(20000))\n",
        "\n",
        "print()\n",
        "print(\"--------------------------------\")\n",
        "print()\n",
        "\n",
        "print(\"Long True Articles (>2000 words):\",\n",
        "      len(true_df[true_df['word_count'] > 2000]))\n",
        "\n",
        "print(true_df[true_df['word_count'] > 2000]['text'].head(20000))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute word counts\n",
        "fake_df['word_count'] = fake_df['text'].str.split().str.len()\n",
        "true_df['word_count'] = true_df['text'].str.split().str.len()\n",
        "\n",
        "# Find the longest article length across both datasets\n",
        "max_len = max(fake_df['word_count'].max(), true_df['word_count'].max())\n",
        "\n",
        "fake_df = fake_df.reset_index(drop=True)\n",
        "true_df = true_df.reset_index(drop=True)\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "# Plot line for Fake\n",
        "plt.plot(fake_df.index, fake_df['word_count'],\n",
        "         color='red', alpha=0.6, label='Fake Articles')\n",
        "\n",
        "# Plot line for True\n",
        "plt.plot(true_df.index, true_df['word_count'],\n",
        "         color='blue', alpha=0.6, label='True Articles')\n",
        "\n",
        "# Labels and formatting\n",
        "plt.title(f\"Article Lengths (0–{max_len} words)\")\n",
        "plt.xlabel(\"Article Index\")\n",
        "plt.ylabel(\"Word Count (words)\")\n",
        "plt.ylim(0, max_len)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ALReup1k3EK"
      },
      "source": [
        "## Missing Value Analysis (Kashvi Vijay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE4v9mCqXwc2"
      },
      "outputs": [],
      "source": [
        "#KASHVI VIJAY - MISSING VAL ANALYSIS\n",
        "# check if the values in any features are null\n",
        "nan_count_true = true_df.isnull().sum()\n",
        "nan_count_fake = fake_df.isnull().sum()\n",
        "print(\"True NaN Count:\\n\", nan_count_true)\n",
        "print(\"\\n\")\n",
        "print(\"Fake NaN Count:\\n\", nan_count_fake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ne92Qw7uJX5"
      },
      "source": [
        "## Unique Words (Kashvi Vijay)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB1TV_2YuoRb"
      },
      "outputs": [],
      "source": [
        "# Added a column to each row for all the unique words in text\n",
        "true_df['unique_words'] = true_df['text'].apply(lambda x: set(re.findall(r\"\\b[\\w']+\\b\", str(x).lower())))\n",
        "\n",
        "# Add a column with the set of unique words for each row in fake_df\n",
        "fake_df['unique_words'] = fake_df['text'].apply(lambda x: set(re.findall(r\"\\b[\\w']+\\b\", str(x).lower())))\n",
        "true_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2xibm18ptEq"
      },
      "source": [
        "## English Detection (Kashvi Vijay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50cb5393"
      },
      "outputs": [],
      "source": [
        "def is_english(word_set, threshold=0.75, min_freq=1.5):\n",
        "    \"\"\"\n",
        "    Returns True if enough words in word_set are common in English.\n",
        "\n",
        "    threshold = proportion of words that must be English\n",
        "    min_freq  = minimum Zipf frequency for a word to count as English\n",
        "    \"\"\"\n",
        "    if not word_set:\n",
        "        return False\n",
        "\n",
        "    english_word_count = sum(\n",
        "        1 for word in word_set if zipf_frequency(word, 'en') > min_freq\n",
        "    )\n",
        "    return (english_word_count / len(word_set)) >= threshold\n",
        "\n",
        "\n",
        "# Apply in parallel\n",
        "with Pool() as pool:\n",
        "    true_df['is_english'] = pool.map(is_english, true_df['unique_words'])\n",
        "    fake_df['is_english'] = pool.map(is_english, fake_df['unique_words'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL0DSjdlpv-F"
      },
      "source": [
        "### Visualization: English vs Non-English (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smnxZADMPYUt"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10,5), sharey=True)\n",
        "\n",
        "# True articles\n",
        "true_df['is_english'].value_counts().plot(kind=\"bar\", ax=ax[0], color=[\"skyblue\", \"lightgray\"])\n",
        "ax[0].set_title(\"True Articles - English vs Non-English\")\n",
        "ax[0].set_xticklabels([\"English\",\"Not English\"], rotation=0)\n",
        "ax[0].set_ylabel(\"Count\")\n",
        "\n",
        "# Fake articles\n",
        "fake_df['is_english'].value_counts().plot(kind=\"bar\", ax=ax[1], color=[\"salmon\", \"lightgray\"])\n",
        "ax[1].set_title(\"Fake Articles - English vs Non-English\")\n",
        "ax[1].set_xticklabels([\"English\",\"Not English\"], rotation=0)\n",
        "\n",
        "plt.suptitle(\"Language Distribution by Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYiYSNLfqBkf"
      },
      "source": [
        "## Sentiment Analysis (Kashvi Vijay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "195823fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# performed sentiment analysis on a smaller sample of 80 to assess whether\n",
        "# there is a relationship between the sentiment and the veracity of an article\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    return analyzer.polarity_scores(text)\n",
        "\n",
        "true_sample = true_df['text'].head(20)\n",
        "fake_sample = fake_df['text'].head(20)\n",
        "\n",
        "with Pool() as pool:\n",
        "    true_sentiments = pool.map(analyze_sentiment, true_sample)\n",
        "    fake_sentiments = pool.map(analyze_sentiment, fake_sample)\n",
        "\n",
        "true_sample_sentiment = true_sample.to_frame()\n",
        "true_sample_sentiment['compound_sentiment'] = [s['compound'] for s in true_sentiments]\n",
        "\n",
        "fake_sample_sentiment = fake_sample.to_frame()\n",
        "fake_sample_sentiment['compound_sentiment'] = [s['compound'] for s in fake_sentiments]\n",
        "\n",
        "print(\"True sample with sentiment:\")\n",
        "display(true_sample_sentiment)\n",
        "\n",
        "print(\"\\nFake sample with sentiment:\")\n",
        "display(fake_sample_sentiment)\n",
        "# close to -1 is negative, close to 1 is positive, and close to 0 is neutral\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGHCrNbGqIhV"
      },
      "source": [
        "### Visualization: Distribution of Sentiment Scores (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKJWBrvQPbwk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(true_sample_sentiment['compound_sentiment'], bins=10, alpha=0.6, label='True', color='skyblue')\n",
        "plt.hist(fake_sample_sentiment['compound_sentiment'], bins=10, alpha=0.6, label='Fake', color='salmon')\n",
        "plt.xlabel(\"Compound Sentiment Score (-1 = Neg, 0 = Neutral, 1 = Pos)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Sentiment Scores (True vs Fake, Sample)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A15zjbe7qZPM"
      },
      "source": [
        "### Visualization: Distribution/boxplot for Compound Sentiment Scores (Kashvi Vijay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88e7cefe"
      },
      "outputs": [],
      "source": [
        "#KASHVI - visualization of sentiment analysis\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Histogram of compound sentiment for true sample\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(true_sample_sentiment['compound_sentiment'], bins=20, kde=True, color='blue')\n",
        "plt.title('Distribution of Compound Sentiment for True News Sample')\n",
        "plt.xlabel('Compound Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Histogram of compound sentiment for fake sample\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(fake_sample_sentiment['compound_sentiment'], bins=20, kde=True, color='red')\n",
        "plt.title('Distribution of Compound Sentiment for Fake News Sample')\n",
        "plt.xlabel('Compound Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "combined_sentiment = pd.concat([\n",
        "    true_sample_sentiment[['compound_sentiment']].assign(News_Type='True Sample'),\n",
        "    fake_sample_sentiment[['compound_sentiment']].assign(News_Type='Fake Sample')\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x='News_Type', y='compound_sentiment', data=combined_sentiment, palette=['blue', 'red'])\n",
        "plt.title('Box Plot of Compound Sentiment for True and Fake News Samples')\n",
        "plt.xlabel('News Type')\n",
        "plt.ylabel('Compound Sentiment Score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTNa368XIX3x"
      },
      "source": [
        "**MORE SENTIMENT ANALYSIS (OUSMAN BAH)**\n",
        "\n",
        "TO VERIFY IF FAKE AND TRUE ARTICLES ARE BIASED TOWARDS A SPECIFIC SUBJECT\n",
        "\n",
        "CONCLUSION: FAKE TEXT CONTAINS ALOT OF DONALD AND EVEN WHICH IS NOT AS FREQUENT IN THE TRUE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6wBaRT0IIcc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Step 1: Vectorize Text (get top words) ---\n",
        "vectorizer = CountVectorizer(max_features=20)  # Top 20 frequent words\n",
        "true_counts = vectorizer.fit_transform(true_df['text'])\n",
        "true_freq = pd.DataFrame(true_counts.toarray(), columns=vectorizer.get_feature_names_out()).sum()\n",
        "\n",
        "fake_counts = vectorizer.fit_transform(fake_df['text'])\n",
        "fake_freq = pd.DataFrame(fake_counts.toarray(), columns=vectorizer.get_feature_names_out()).sum()\n",
        "\n",
        "# --- Step 2: Combine for Comparison ---\n",
        "freq_df = pd.DataFrame({\n",
        "    'True News': true_freq,\n",
        "    'Fake News': fake_freq\n",
        "}).fillna(0)\n",
        "\n",
        "# --- Step 3: Plot Side-by-Side Bar Chart ---\n",
        "plt.figure(figsize=(12,6))\n",
        "freq_df.head(15).plot(kind='bar', width=0.8)\n",
        "plt.title('Top Word Frequency Comparison: True vs Fake News')\n",
        "plt.xlabel('Top Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiGE7sUuVIF"
      },
      "source": [
        "## Subject Finding Analysis (Lin Zhang)\n",
        "\n",
        "**What I found:**\n",
        "There is zero overlap between true-news subjects and fake-news subjects, except for politics which still impacts most of the articles\n",
        "\n",
        "**Why this matters:** model cheating, model can learn the pattern recognition and not actually doing the content learning. data leakage if we dont do anything to it.\n",
        "\n",
        "**What I suggest**: Remove suject column and perform content-only training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npMRSrV4rYAA"
      },
      "outputs": [],
      "source": [
        "# Subject findings\n",
        "print(f\"True News: {len(true_df):,} articles\")\n",
        "print(f\"Fake News: {len(fake_df):,} articles\")\n",
        "print(\"true_subjects = true_df['subject'].value_counts()\")\n",
        "print(\"fake_subjects = fake_df['subject'].value_counts()\")\n",
        "\n",
        "true_df_cleaned = true_df\n",
        "fake_df_cleaned = fake_df\n",
        "true_subjects = true_df['subject'].value_counts()\n",
        "fake_subjects = fake_df['subject'].value_counts()\n",
        "\n",
        "print(f\"\\nTrue News Subject\")\n",
        "for subject, count in true_subjects.items():\n",
        "  percentage = (count/len(true_df))*100\n",
        "  print(f\" {subject}: {count:,} articles ({percentage:.1f}%)\")\n",
        "print(f\"\\nFake News Subject\")\n",
        "for subject, count in fake_subjects.items():\n",
        "  percentage = (count/len(fake_df))*100\n",
        "  print(f\" {subject}: {count:,} articles ({percentage:.1f}%)\")\n",
        "\n",
        "\n",
        "true_unique_subjects = true_df['subject'].unique()\n",
        "fake_unique_subjects = fake_df['subject'].unique()\n",
        "\n",
        "true_subject_set = set(true_subjects.index)\n",
        "fake_subject_set = set(fake_subjects.index)\n",
        "\n",
        "common_subject = true_subject_set.intersection(fake_subject_set)\n",
        "true_only_subject = true_subject_set-fake_subject_set\n",
        "fake_only_subject = fake_subject_set-true_subject_set\n",
        "\n",
        "#graph:\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Get all unique subjects\n",
        "subjects = list(true_subjects.index) + list(fake_subjects.index)\n",
        "true_counts = [true_subjects.get(s, 0) for s in subjects]\n",
        "fake_counts = [fake_subjects.get(s, 0) for s in subjects]\n",
        "\n",
        "#bar chart\n",
        "x = np.arange(len(subjects))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, true_counts, width, label='True News', color='#2E8B57', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, fake_counts, width, label='Fake News', color='#DC143C', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Subject Categories')\n",
        "ax.set_ylabel('Number of Articles')\n",
        "ax.set_title('Subject Distribution: True vs Fake News')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(subjects, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    if height > 0:\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 100,\n",
        "                f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    if height > 0:\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 100,\n",
        "                f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"True:\", len(true_df_cleaned))\n",
        "print(\"Fake:\", len(fake_df_cleaned))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T9sWSgjvR5d"
      },
      "source": [
        "## Duplication Analysis (Lin Zhang)\n",
        "\n",
        "**What is this about:** This analysis is to go over the duplication articles in both true and fake news dataset.\n",
        "\n",
        "**What I found:** There is total 21,417 articles in true news, and 23,481 articles in fake news.\n",
        "After inspecting the duplications, I found theres 427 articles in true news are duplicated, and 10,671 articles in fake news are duplicated.\n",
        "\n",
        "**Why this matters:** It matters because duplicated articles does not add any value to our model. There's no benefit for keeping fuplicated articles\n",
        "\n",
        "**What I suggest**: I suggest we drop all the duplicated articles to keep our dataset with only unique articles. After duplication we will ending with 21,197 true articles, 17,908 fake articles. In ratio it is 1.18:1(true:fake) which will not effect much on later trainings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqzQXbJdv09P"
      },
      "outputs": [],
      "source": [
        "true_duplicates = true_df_cleaned.duplicated(subset=['title', 'text'], keep=False)\n",
        "fake_duplicates = fake_df_cleaned.duplicated(subset=['title', 'text'], keep=False)\n",
        "\n",
        "true_duplicate_count = true_duplicates.sum()\n",
        "fake_duplicate_count = fake_duplicates.sum()\n",
        "\n",
        "true_original = len(true_df)\n",
        "fake_original = len(fake_df)\n",
        "\n",
        "# remove duplicates from cleaned dataframe\n",
        "true_df_cleaned.drop_duplicates(subset=['title', 'text'], keep='first', inplace=True)\n",
        "fake_df_cleaned.drop_duplicates(subset=['title', 'text'], keep='first', inplace=True)\n",
        "\n",
        "true_after = len(true_df_cleaned)\n",
        "fake_after = len(fake_df_cleaned)\n",
        "\n",
        "\n",
        "\n",
        "# graph\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
        "fig.suptitle('Duplicate Removal - Before vs After', fontsize=16, fontweight='bold')\n",
        "\n",
        "datasets = ['True News', 'Fake News']\n",
        "before_counts = [true_original, fake_original]\n",
        "after_counts = [true_after, fake_after]\n",
        "\n",
        "x = np.arange(len(datasets))\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, before_counts, width, label='Before Duplicate Removal', color='grey', alpha=0.8)\n",
        "bars2 = ax.bar(x + width/2, after_counts, width, label='After Duplicate Removal', color='green', alpha=0.8)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Dataset')\n",
        "ax.set_ylabel('Number of Articles')\n",
        "ax.set_title('Article Count: Before vs After Duplicate Removal')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(datasets)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# load data in graph\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 100,\n",
        "                f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "ax.text(0, before_counts[0] + 500, f'Duplicates: {true_duplicate_count:,}',\n",
        "        ha='center', va='bottom', fontweight='bold', color='red')\n",
        "ax.text(1, before_counts[1] + 500, f'Duplicates: {fake_duplicate_count:,}',\n",
        "        ha='center', va='bottom', fontweight='bold', color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"BEFORE duplicate removal:\")\n",
        "print(f\"  True News: {true_original:,} articles\")\n",
        "print(f\"  Fake News: {fake_original:,} articles\")\n",
        "print(f\"  Ratio: {true_original/fake_original:.3f}:1 (True:Fake)\")\n",
        "\n",
        "print(f\"\\nAFTER duplicate removal:\")\n",
        "print(f\"  True News: {true_after:,} articles\")\n",
        "print(f\"  Fake News: {fake_after:,} articles\")\n",
        "print(f\"  Ratio: {true_after/fake_after:.3f}:1 (True:Fake)\")\n",
        "\n",
        "print(f\"\\nDUPLICATES REMOVED:\")\n",
        "print(f\"  True News: {true_duplicate_count:,} duplicates ({(true_duplicate_count/true_original*100):.2f}%)\")\n",
        "print(f\"  Fake News: {fake_duplicate_count:,} duplicates ({(fake_duplicate_count/fake_original*100):.2f}%)\")\n",
        "\n",
        "print(\"Original files:\")\n",
        "print(f\"True.csv: {len(true_df):,} articles\")\n",
        "print(f\"Fake.csv: {len(fake_df):,} articles\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0g9NJQ3XDkE"
      },
      "source": [
        "## Date Analysis (Sanskriti Khadka)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5_JYMpP0b98"
      },
      "source": [
        "**Date Standardization / Analysis:**\n",
        "\n",
        "In this part of the analysis, I standardized the article dates and checked for invalid or missing values, which were very rare. The visualizations showed strong temporal clustering, with true news articles concentrated in late 2017 while fake news was more spread out. Because this could create bias, I suggest removing dates from the modeling step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjbXXaK3HuRR"
      },
      "outputs": [],
      "source": [
        "# Date format standardization / Analysis\n",
        "\n",
        "# Date Standardization\n",
        "true_df['date_clean'] = pd.to_datetime(true_df['date'], format='mixed', errors='coerce')\n",
        "fake_df['date_clean'] = pd.to_datetime(fake_df['date'], format='mixed', errors='coerce')\n",
        "\n",
        "print(\"Date standardization complete.\")\n",
        "\n",
        "\n",
        "print(f\"True articles: {true_df['date_clean'].notna().sum()} valid dates\")\n",
        "print(f\"Fake articles: {fake_df['date_clean'].notna().sum()} valid dates\")\n",
        "\n",
        "print(f\"True invalid dates: {true_df['date_clean'].isna().sum()}\")\n",
        "print(f\"Fake invalid dates: {fake_df['date_clean'].isna().sum()}\")\n",
        "\n",
        "# Valid vs Invalid Dates\n",
        "valid_counts = [\n",
        "    true_df['date_clean'].notna().sum(),\n",
        "    fake_df['date_clean'].notna().sum()\n",
        "]\n",
        "invalid_counts = [\n",
        "    true_df['date_clean'].isna().sum(),\n",
        "    fake_df['date_clean'].isna().sum()\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "bars_valid = sns.barplot(\n",
        "    x=['True News', 'Fake News'],\n",
        "    y=valid_counts,\n",
        "    color='green',\n",
        "    label='Valid Dates'\n",
        ")\n",
        "bars_invalid = sns.barplot(\n",
        "    x=['True News', 'Fake News'],\n",
        "    y=invalid_counts,\n",
        "    color='red',\n",
        "    bottom=valid_counts,\n",
        "    label='Invalid Dates'\n",
        ")\n",
        "\n",
        "# Timeline Distribution\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot(true_df['date_clean'].dropna(), bins=50, color='blue', label='True News', alpha=0.6)\n",
        "sns.histplot(fake_df['date_clean'].dropna(), bins=50, color='red', label='Fake News', alpha=0.6)\n",
        "plt.title(\"Timeline Distribution of Articles\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Monthly Trends\n",
        "true_month = true_df['date_clean'].dt.to_period(\"M\").value_counts().sort_index()\n",
        "fake_month = fake_df['date_clean'].dt.to_period(\"M\").value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(true_month.index.astype(str), true_month.values, label='True News', color='blue')\n",
        "plt.plot(fake_month.index.astype(str), fake_month.values, label='Fake News', color='red')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Monthly Article Counts\")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM0OU70drz6q"
      },
      "source": [
        "## Source Analysis (Sankriti Khadka)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZJlJn6Y03eQ"
      },
      "source": [
        "**Source Analysis:**\n",
        "\n",
        "In this part of the analysis, I looked at which sources were mentioned in the articles. Real news mostly cited professional outlets like Reuters and Independent, while fake news relied more on social media platforms like Twitter, YouTube, and Facebook. This shows clear differences in sourcing patterns, but I suggest removing the top sources from the dataset since they could create bias and make the model overfit to specific names instead of content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMFfj4si_o5u"
      },
      "outputs": [],
      "source": [
        "#Sanskriti Khadka - Source Analysis\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def find_sources(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "\n",
        "    sources = [\n",
        "        'CNN', 'Fox News', 'MSNBC', 'NBC', 'ABC', 'CBS', 'PBS',\n",
        "        'New York Times', 'Washington Post', 'Wall Street Journal', 'USA Today', 'Los Angeles Times', 'Chicago Tribune',\n",
        "        'Reuters', 'Associated Press', 'AP', 'Bloomberg', 'UPI',\n",
        "        'BBC', 'Guardian', 'Telegraph', 'Independent', 'Sky News',\n",
        "        'Politico', 'Huffington Post', 'BuzzFeed', 'Vox', 'Axios', 'Daily Beast', 'Slate', 'Salon', 'The Hill',\n",
        "        'Breitbart', 'Daily Wire', 'National Review', 'Weekly Standard', 'Washington Examiner', 'New York Post',\n",
        "        'The Nation', 'Mother Jones', 'Daily Kos', 'ThinkProgress',\n",
        "        'Daily Mail', 'Sun', 'Mirror', 'Express',\n",
        "        'Twitter', 'Facebook', 'Instagram', 'YouTube', 'TikTok',\n",
        "        'Pentagon', 'FBI', 'CIA', 'State Department', 'Justice Department', 'Department of Defense'\n",
        "    ]\n",
        "\n",
        "    found = []\n",
        "    text_str = str(text)\n",
        "\n",
        "    for source in sources:\n",
        "        pattern = r'\\b' + re.escape(source) + r'\\b'\n",
        "        if re.search(pattern, text_str, re.IGNORECASE):\n",
        "            found.append(source)\n",
        "\n",
        "    return found\n",
        "\n",
        "print(\"ANALYZING SAMPLE OF DATASET...\")\n",
        "\n",
        "# random sample\n",
        "fake_sample = fake_df.sample(n=3000, random_state=1234)\n",
        "true_sample = true_df.sample(n=3000, random_state=1234)\n",
        "\n",
        "fake_sources = []\n",
        "for text in fake_sample['text']:\n",
        "    fake_sources.extend(find_sources(text))\n",
        "\n",
        "real_sources = []\n",
        "for text in true_sample['text']:\n",
        "    real_sources.extend(find_sources(text))\n",
        "\n",
        "print(f\"\\nTOP 10 SOURCES IN FAKE articles (sample):\")\n",
        "fake_counts = Counter(fake_sources)\n",
        "for source, count in fake_counts.most_common(10):\n",
        "    print(f\"{source}: {count}\")\n",
        "\n",
        "print(f\"\\nTOP 10 SOURCES IN REAL articles (sample):\")\n",
        "real_counts = Counter(real_sources)\n",
        "for source, count in real_counts.most_common(10):\n",
        "    print(f\"{source}: {count}\")\n",
        "\n",
        "# Data Visulization\n",
        "fake_top10 = pd.DataFrame(fake_counts.most_common(10), columns=[\"Source\", \"Count\"])\n",
        "real_top10 = pd.DataFrame(real_counts.most_common(10), columns=[\"Source\", \"Count\"])\n",
        "\n",
        "# Fake News Sources\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"Count\", y=\"Source\", data=fake_top10, hue=\"Source\", dodge=False, legend=False, palette=\"Reds_r\")\n",
        "plt.title(\"Top 10 Sources in Fake News (Sample)\")\n",
        "plt.xlabel(\"Mentions\")\n",
        "plt.ylabel(\"Source\")\n",
        "plt.show()\n",
        "\n",
        "# Real News Sources\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"Count\", y=\"Source\", data=real_top10, hue=\"Source\", dodge=False, legend=False, palette=\"Blues_r\")\n",
        "plt.title(\"Top 10 Sources in Real News (Sample)\")\n",
        "plt.xlabel(\"Mentions\")\n",
        "plt.ylabel(\"Source\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxHvIizryalK"
      },
      "source": [
        "## URL Analysis (Nancy Huang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UkhkNzuvXDf"
      },
      "source": [
        "**URL/Link Analysis Conclusion**\n",
        "\n",
        "Urls are unique to the *FAKE NEWS* dataset.\n",
        "t.co showed up 2,055 times (shortened version of twitter.com, now known as X), this suggests that many fake news articles pulled in tweets directly, quoted them, or used Twitter (X) as a primary \"source.\" The pattern that surfaces is the presence of social media links strongly correlates with fake news. *TRUE NEWS* has no Urls.\n",
        "\n",
        "**My suggestion/approach:** Replace all Urls with a generic <URL> token when cleaning, we should not leave any raw Urls because this risks overfitting or we can add a numeric value to capture the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qalya9NGzs8f"
      },
      "outputs": [],
      "source": [
        "fake_df['domains'] = fake_df['text'].str.extract(r'://(?:www\\.)?([^/\\s]+)')\n",
        "print(\"Highest links (domains) in Fake News dataset: \")\n",
        "print(fake_df['domains'].value_counts().head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCUZFu7Az-G0"
      },
      "outputs": [],
      "source": [
        "true_df['domains'] = true_df['text'].str.extract(r'://(?:www\\.)?([^/\\s]+)')\n",
        "print(\"Top links (domains) in Fake News dataset: \")\n",
        "print(true_df['domains'].value_counts().head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjVtsfAusAvW"
      },
      "source": [
        "## Source Analysis with Focus on Twitter (Adriena Jiang)\n",
        "\n",
        "**Findings:**\n",
        "\n",
        "Both real and fake news have an abundance of news that comes from twitter. Twitter news is more likely fake than real.\n",
        "\n",
        "**Meaning:**\n",
        "\n",
        "It may matter for model cheating. It may learn a pattern and lean torwards false. Potential data leakage.\n",
        "\n",
        "**Suggestions**:\n",
        "\n",
        "Even though it skews torwards false, we should keep twitter mentions unlike Reuter because there is still a good portion of twitter news that lies in the true news data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UnryTMa8VJj"
      },
      "outputs": [],
      "source": [
        "# Further analysis on twitter occurences.\n",
        "\n",
        "# Look for the number of articles that mention 'twitter'\n",
        "real_twitter = true_df['text'].str.contains(r'twitter', case=False, na=False).sum()\n",
        "fake_twitter = fake_df['text'].str.contains(r'twitter', case=False, na=False).sum()\n",
        "print(f\"Twitter in Real News: {real_twitter}\")\n",
        "print(f\"Twitter in Fake News: {fake_twitter}\")\n",
        "\n",
        "# Plot the statistic above for visualization\n",
        "plt.bar([\"Real News\", \"Fake News\"], [real_twitter, fake_twitter], color=[\"blue\",\"red\"])\n",
        "plt.title(\"Articles Mentioning Twitter\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.show()\n",
        "\n",
        "# Percentage of each dataset contains the word \"twitter\"\n",
        "real_share = real_twitter / len(true_df) * 100\n",
        "fake_share = fake_twitter / len(fake_df) * 100\n",
        "print(f\"Real News mentioning Twitter: {real_share:.2f}%\")\n",
        "print(f\"Fake News mentioning Twitter: {fake_share:.2f}%\")\n",
        "\n",
        "# Proportion of mentions between Real vs Fake\n",
        "plt.pie([real_share, fake_share], labels=[\"Real News\",\"Fake News\"],\n",
        "        autopct=\"%1.1f%%\", colors=[\"blue\",\"red\"], startangle=90)\n",
        "plt.title(\"Share of Articles Mentioning Twitter\")\n",
        "plt.show()\n",
        "\n",
        "# Look at some of the articles that mention twitter.\n",
        "print(\"\\nReal News Examples:\")\n",
        "for text in true_df[true_df['text'].str.contains(\"twitter\", case=False, na=False)]['text'].sample(5, random_state=42):\n",
        "    print(\"\\n--- Article ---\")\n",
        "    print(text)\n",
        "print(\"\\nFake News Examples:\")\n",
        "for text in fake_df[fake_df['text'].str.contains(\"twitter\", case=False, na=False)]['text'].sample(5, random_state=42):\n",
        "    print(\"\\n--- Article ---\")\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOQ52BV7wSFh"
      },
      "source": [
        "## Author Bylines Analysis (Nancy Huang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Hm-0ycyFrD"
      },
      "source": [
        "**Author Byline Takeaways**\n",
        "\n",
        "The number of bylines for both Fake and True news are very low. *TRUE NEWS*  has 0 bylines, likely because many true news datasets are cleaned to keep only the articles body so theres consistency. Whereas, *FAKE NEWS* datasets are often taken from websites or social media posts.\n",
        "\n",
        "**My sugessted solution:** We should remove bylines from all the fake news articles so our model can learn from the actual content instead of the inconsistencies in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNe78t7ivY9t"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "fake_df['byline'] = fake_df['text'].str.extract(r'^\\s*By\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)')\n",
        "true_df['byline'] = true_df['text'].str.extract(r'^\\s*By\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)')\n",
        "print(\"Fake with byline:\", fake_df['byline'].notna().sum(), \"out of\", len(fake_df))\n",
        "print(\"True with byline:\", true_df['byline'].notna().sum(), \"out of\", len(true_df))\n",
        "print(\"Top fake bylines:\")\n",
        "print(fake_df['byline'].value_counts().head(20))\n",
        "print(\"\\nTop true bylines:\")\n",
        "print(true_df['byline'].value_counts().head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVoqsCywvNFa"
      },
      "source": [
        "## Stop Word Analysis (Sanskriti Khadka)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S6InCPi1qPP"
      },
      "source": [
        "In this part of the analysis, I looked at stop words, common words like the, is, and, etc. Since these words don't add much meaning, I checked their frequencies and removed them to clean up the text. I also compared article lengths before and after removal to see how much noise was reduced. I suggest using the cleaned version for modeling since it highlights meaningful words, but keeping the original text for context and interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDRLhhr8q2ci"
      },
      "outputs": [],
      "source": [
        "# Stop Word Anaylsis\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def get_stopword_counts(texts):\n",
        "    words = \" \".join(texts.astype(str)).lower().split()\n",
        "    stopword_list = [w for w in words if w in stop_words]\n",
        "    return Counter(stopword_list)\n",
        "\n",
        "# Count stopwords in True and Fake datasets\n",
        "true_stop_counts = get_stopword_counts(true_df['text'])\n",
        "fake_stop_counts = get_stopword_counts(fake_df['text'])\n",
        "\n",
        "# Top 20 stop words for each\n",
        "true_top = pd.DataFrame(true_stop_counts.most_common(20), columns=[\"Word\",\"Count\"])\n",
        "fake_top = pd.DataFrame(fake_stop_counts.most_common(20), columns=[\"Word\",\"Count\"])\n",
        "\n",
        "# Plot comparison\n",
        "fig, axes = plt.subplots(1,2, figsize=(14,6), sharey=True)\n",
        "\n",
        "sns.barplot(data=true_top, x=\"Count\", y=\"Word\", ax=axes[0], color=\"blue\")\n",
        "axes[0].set_title(\"Top Stopwords in True News\")\n",
        "\n",
        "sns.barplot(data=fake_top, x=\"Count\", y=\"Word\", ax=axes[1], color=\"red\")\n",
        "axes[1].set_title(\"Top Stopwords in Fake News\")\n",
        "\n",
        "plt.suptitle(\"Stop Word Frequency (Before Removal)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSJfWzCyivQI"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGNcTNK4HDed"
      },
      "source": [
        "## Subject Removal Implementation (Lin Zhang)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWqbGqd0pi8_"
      },
      "outputs": [],
      "source": [
        "#COPY DATA FOR VISUALIZATION BEFORE AND AFTER\n",
        "true_before = true_df_cleaned[\"text\"].copy()\n",
        "fake_before = fake_df_cleaned[\"text\"].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU6n1gAJHIU-"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"BEFORE subject removal:\")\n",
        "print(f\"True News: {len(true_df):,} articles, {len(true_df.columns)} columns\")\n",
        "print(f\"Fake News: {len(fake_df):,} articles, {len(fake_df.columns)} columns\")\n",
        "print(f\"Columns: {list(true_df.columns)}\")\n",
        "\n",
        "\n",
        "true_df_cleaned = true_df.drop('subject', axis=1)\n",
        "fake_df_cleaned = fake_df.drop('subject', axis=1)\n",
        "\n",
        "print(\"AFTER subject removal:\")\n",
        "print(f\"True News: {len(true_df_cleaned):,} articles, {len(true_df_cleaned.columns)} columns\")\n",
        "print(f\"Fake News: {len(fake_df_cleaned):,} articles, {len(fake_df_cleaned.columns)} columns\")\n",
        "print(f\"Columns: {list(true_df_cleaned.columns)}\")\n",
        "\n",
        "\n",
        "true_df_cleaned.to_csv('True_cleaned.csv', index=False)\n",
        "true_df_cleaned.to_csv('Fake_cleaned.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHlkJmeluGNx"
      },
      "source": [
        "**Subject Removal Completed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIo07Nqrsals"
      },
      "source": [
        "## Clean `Reuter` mentions in `True.csv` Data Set (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7u7YpuH7hO-"
      },
      "outputs": [],
      "source": [
        "REUTERS_PATTERN = r'\\(?\\s*reuters\\s*\\)?\\s*(?:[-–—:])?'\n",
        "\n",
        "def remove_reuters(df):\n",
        "    # work on a copy and avoid chained-assignment warnings\n",
        "    out = df.copy()\n",
        "    out.loc[:, 'text'] = (\n",
        "        out['text']\n",
        "          .astype(str)\n",
        "          # Remove Reuter Mentions\n",
        "          .str.replace(REUTERS_PATTERN, ' ', regex=True, flags=re.I)\n",
        "          # Collapse Spaces\n",
        "          .str.replace(r'\\s+', ' ', regex=True)\n",
        "          .str.strip()\n",
        "    )\n",
        "    return out\n",
        "\n",
        "# Remove Reuters\n",
        "true_df_cleaned = remove_reuters(true_df_cleaned)\n",
        "fake_df_cleaned = remove_reuters(fake_df_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9aTdJP27kAL"
      },
      "outputs": [],
      "source": [
        "# Inspect data frame.\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# sample for speed\n",
        "fake_after_sample = fake_df_cleaned.sample(n=3000, random_state=1234)\n",
        "true_after_sample = true_df_cleaned.sample(n=3000, random_state=1234)\n",
        "\n",
        "fake_after_sources = []\n",
        "for text in fake_after_sample['text']:\n",
        "    fake_after_sources.extend(find_sources(text))\n",
        "\n",
        "real_after_sources = []\n",
        "for text in true_after_sample['text']:\n",
        "    real_after_sources.extend(find_sources(text))\n",
        "\n",
        "fake_after_counts = Counter(fake_after_sources)\n",
        "real_after_counts = Counter(real_after_sources)\n",
        "\n",
        "fake_after_top10 = pd.DataFrame(fake_after_counts.most_common(10), columns=[\"Source\", \"Count\"])\n",
        "real_after_top10 = pd.DataFrame(real_after_counts.most_common(10), columns=[\"Source\", \"Count\"])\n",
        "\n",
        "#Fake after clean\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"Count\", y=\"Source\", data=fake_after_top10, hue=\"Source\",\n",
        "            dodge=False, legend=False, palette=\"Reds_r\")\n",
        "plt.title(\"Top 10 Sources in Fake News (AFTER Cleaning)\")\n",
        "plt.xlabel(\"Mentions\")\n",
        "plt.ylabel(\"Source\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#real afetr clean\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"Count\", y=\"Source\", data=real_after_top10, hue=\"Source\",\n",
        "            dodge=False, legend=False, palette=\"Blues_r\")\n",
        "plt.title(\"Top 10 Sources in Real News (AFTER Cleaning)\")\n",
        "plt.xlabel(\"Mentions\")\n",
        "plt.ylabel(\"Source\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZbJZZaQu5o4"
      },
      "source": [
        "## URL Tokenization (Nancy Huang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RRtIdbQkrnD"
      },
      "source": [
        "This block standardizes all URLs in the Fake and True dataset by replacing them with a generic \"< URL >\" token. The goal is to prevent downstream models from memorizing a specific domain/link.\n",
        "\n",
        "What I did & Why:\n",
        "1. I made a copy of the \"text\" column in the dataset to keep the original raw text.\n",
        "2. PRE: This step is for detecting which row contains Urls before masking the datasets.\n",
        "3. MASKING: This is when the Urls in the text are replaced with the generic URL token.\n",
        "4. POST: This rechecks which rows contains the URL token after the masking process.\n",
        "5. RESIDUALS: This checks that there are no raw Urls left in the datasets\n",
        "6. Printed results\n",
        "\n",
        "Outcome/Results: All Urls are normalized to \"< URL > \" which ensures consistent for modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onNQFxsJuR9C"
      },
      "source": [
        "Results:\n",
        "\n",
        "*FAKE NEWS DATASET*\n",
        "\n",
        "**Pre:** 89.69% of rows contained at least one raw Url. This is very high which means the fake news article in our dataset almost always include links.\n",
        "\n",
        "**Post:** After masking it is still 89.69% which makes sense because the < URL> token exists which means every Url got replaced, no Urls were lost, and no extra ones were created.\n",
        "\n",
        "*TRUE NEWS DATASET*\n",
        "\n",
        "**Pre:** 6.6% of roaws had a raw Url which means the true news articles rarely included links.\n",
        "\n",
        "**Post:** After masking, it was still 6.6% which means the Url link were successfully replaced.\n",
        "\n",
        "We know that the Post results are factual (that the links were successfuly replaced) due to the residual raw Url data. It shows that there are no leftover raw Urls in the text after masking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alpbbKa7djud"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# URL patterns\n",
        "PROTO_URL = r'https?://[^\\s)>\\]}]+'\n",
        "WWW_URL   = r'www\\.[^\\s)>\\]}]+'\n",
        "BARE_DOM  = r'\\b(?:[a-z0-9-]+\\.)+[a-z]{2,}(?:/[^\\s)>\\]}]+)?'\n",
        "URL_REGEX = re.compile(f'(?:{PROTO_URL})|(?:{WWW_URL})|(?:{BARE_DOM})', flags=re.IGNORECASE)\n",
        "\n",
        "ORG_WORDS = [\n",
        "    r'reuters', r'associated press', r'\\bap\\b', r'bbc', r'guardian',\n",
        "    r'fox news', r'cnn', r'msnbc', r'nbc', r'abc', r'cbs',\n",
        "    r'new york times', r'washington post', r'wsj', r'usa today',\n",
        "    r'breitbart', r'daily mail', r'daily wire', r'vox', r'axios',\n",
        "    r'twitter', r'facebook', r'instagram', r'youtube', r'tiktok',\n",
        "    r'pentagon', r'fbi', r'cia', r'state department'\n",
        "]\n",
        "ORG_REGEX = re.compile(r'\\b(' + r'|'.join(ORG_WORDS) + r')\\b', re.IGNORECASE)\n",
        "\n",
        "def clean_text_for_bert(text):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    text = URL_REGEX.sub(' ', text)\n",
        "\n",
        "    text = ORG_REGEX.sub(' ', text)\n",
        "\n",
        "    text = re.sub(r'image via.*?(?=\\.|$)', '', text, flags=re.I)\n",
        "    text = re.sub(r'featured image.*?(?=\\.|$)', '', text, flags=re.I)\n",
        "    text = re.sub(r'getty images', '', text, flags=re.I)\n",
        "    text = re.sub(r'video screen capture', '', text, flags=re.I)\n",
        "    text = re.sub(r'watch:?\\s*video', '', text, flags=re.I)\n",
        "\n",
        "    text = re.sub(r'^\\s*[A-Z\\s/,]{3,50}\\s+[-–—]?\\s*', '', text)\n",
        "\n",
        "    text = re.sub(r'source:.*$', '', text, flags=re.I | re.M)\n",
        "    text = re.sub(r'via:.*$', '', text, flags=re.I | re.M)\n",
        "\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"Cleaning text...\")\n",
        "true_df_cleaned['text_clean'] = true_df_cleaned['text'].apply(clean_text_for_bert)\n",
        "fake_df_cleaned['text_clean'] = fake_df_cleaned['text'].apply(clean_text_for_bert)\n",
        "\n",
        "true_df_cleaned = true_df_cleaned[true_df_cleaned['text_clean'].str.len() > 100]\n",
        "fake_df_cleaned = fake_df_cleaned[fake_df_cleaned['text_clean'].str.len() > 100]\n",
        "\n",
        "print(f\"After cleaning:\")\n",
        "print(f\"True articles: {len(true_df_cleaned)}\")\n",
        "print(f\"Fake articles: {len(fake_df_cleaned)}\")\n",
        "\n",
        "print(\"\\nVerifying no URL tokens:\")\n",
        "print(f\"True has '<URL>': {true_df_cleaned['text_clean'].str.contains('<URL>').sum()}\")\n",
        "print(f\"Fake has '<URL>': {fake_df_cleaned['text_clean'].str.contains('<URL>').sum()}\")\n",
        "\n",
        "print(\"\\nVerifying no ORG tokens:\")\n",
        "print(f\"True has '<ORG>': {true_df_cleaned['text_clean'].str.contains('<ORG>').sum()}\")\n",
        "print(f\"Fake has '<ORG>': {fake_df_cleaned['text_clean'].str.contains('<ORG>').sum()}\")\n",
        "\n",
        "# sample outputs to verify\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE CLEANED FAKE NEWS:\")\n",
        "print(\"=\"*70)\n",
        "print(fake_df_cleaned['text_clean'].iloc[0][:500])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SAMPLE CLEANED TRUE NEWS:\")\n",
        "print(\"=\"*70)\n",
        "print(true_df_cleaned['text_clean'].iloc[0][:500])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HucLadKas5Cb"
      },
      "outputs": [],
      "source": [
        "#VISULAIZE URL Before VS After\n",
        "\n",
        "# BEFORE CLEANING (computed BEFORE applying clean_text_for_bert)\n",
        "before_true_url = true_df_cleaned[\"text\"].str.contains(URL_REGEX).sum()\n",
        "before_fake_url = fake_df_cleaned[\"text\"].str.contains(URL_REGEX).sum()\n",
        "\n",
        "before_true_org = true_df_cleaned[\"text\"].str.contains(ORG_REGEX).sum()\n",
        "before_fake_org = fake_df_cleaned[\"text\"].str.contains(ORG_REGEX).sum()\n",
        "\n",
        "# AFTER CLEANING\n",
        "after_true_url = true_df_cleaned['text_clean'].str.contains('<URL>').sum()\n",
        "after_fake_url = fake_df_cleaned['text_clean'].str.contains('<URL>').sum()\n",
        "\n",
        "after_true_org = true_df_cleaned['text_clean'].str.contains('<ORG>').sum()\n",
        "after_fake_org = fake_df_cleaned['text_clean'].str.contains('<ORG>').sum()\n",
        "\n",
        "\n",
        "compare_df = pd.DataFrame({\n",
        "    \"Token\": [\"URL\", \"URL\", \"ORG\", \"ORG\"],\n",
        "    \"Dataset\": [\"True\", \"Fake\", \"True\", \"Fake\"],\n",
        "    \"Before\": [before_true_url, before_fake_url, before_true_org, before_fake_org],\n",
        "    \"After\":  [after_true_url,  after_fake_url,  after_true_org,  after_fake_org]\n",
        "})\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
        "\n",
        "# BEFORE\n",
        "sns.barplot(ax=axes[0], data=compare_df, x=\"Token\", y=\"Before\", hue=\"Dataset\")\n",
        "axes[0].set_title(\"BEFORE Cleaning: URL & ORG Tokens\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "# AFTER\n",
        "sns.barplot(ax=axes[1], data=compare_df, x=\"Token\", y=\"After\", hue=\"Dataset\")\n",
        "axes[1].set_title(\"AFTER Cleaning: URL & ORG Tokens\")\n",
        "axes[1].set_ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBBwaXEXvGT1"
      },
      "source": [
        "## Dropping Blank Text Data (Ousman Bah)\n",
        "\n",
        "\n",
        "***Fake News Dataset:***\n",
        "\n",
        "**Pre Dropping:** 17,908 rows\n",
        "**Post Dropping:** 17,462 rows\n",
        "\n",
        "**Details:** A total of 446 rows were dropped because their text fields were empty or missing. These rows wouldn’t add value to analysis or model training, so removing them helped keep only meaningful articles.\n",
        "\n",
        "***True News Dataset:***\n",
        "\n",
        "**Pre Dropping:** 21,197 rows\n",
        "**Post Dropping:** 21,196 rows\n",
        "\n",
        "Details: Only 1 row was removed due to blank text. This shows the True dataset was already clean and consistent, requiring almost no data removal for text completeness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFV5fOQTw4lP"
      },
      "outputs": [],
      "source": [
        "#DROPPING BLANK TEXT DATA\n",
        "\n",
        "print(\"True cleaned shape:\", true_df_cleaned.shape)\n",
        "print(\"Fake cleaned shape:\", fake_df_cleaned.shape)\n",
        "\n",
        "# --- Clean Fake dataset ---\n",
        "# Count before\n",
        "print(\"\\nBefore dropping blanks, Fake rows:\", len(fake_df))\n",
        "\n",
        "fake_df_cleaned = fake_df_cleaned.dropna(subset=['text'])\n",
        "fake_df_cleaned = fake_df_cleaned[fake_df_cleaned['text'].str.strip() != \"\"]\n",
        "\n",
        "print(\"Fake_cleaned shape after drop:\", fake_df_cleaned.shape)\n",
        "\n",
        "# Count before\n",
        "print(\"\\nBefore dropping blanks, True rows:\", len(true_df))\n",
        "\n",
        "true_df_cleaned = true_df_cleaned.dropna(subset=['text'])\n",
        "true_df_cleaned = true_df_cleaned[true_df_cleaned['text'].str.strip() != \"\"]\n",
        "\n",
        "\n",
        "# Count after\n",
        "print(\"True_cleaned shape after drop:\", true_df_cleaned.shape)\n",
        "\n",
        "true_before = len(true_df)\n",
        "fake_before = len(fake_df)\n",
        "\n",
        "true_after = len(true_df_cleaned)\n",
        "fake_after = len(fake_df_cleaned)\n",
        "\n",
        "true_before = int(true_before)\n",
        "fake_before = int(fake_before)\n",
        "true_after = int(true_after)\n",
        "fake_after = int(fake_after)\n",
        "drop_compare = pd.DataFrame({\n",
        "    \"Dataset\": [\"True Before\", \"True After\", \"Fake Before\", \"Fake After\"],\n",
        "    \"Count\": [true_before, true_after, fake_before, fake_after]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=drop_compare, x=\"Dataset\", y=\"Count\", palette=\"Blues\")\n",
        "plt.title(\"Before vs After Dropping Blank Text Data\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.xticks(rotation=25)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxRF7n-3CeMe"
      },
      "outputs": [],
      "source": [
        "#Text with Stop Words Ousman Bah.\n",
        "\n",
        "#Added a text with stop word analysis as advised by Abdul.\n",
        "# Create a column to store the text with stopwords (before removing them)\n",
        "for df in (true_df_cleaned, fake_df_cleaned):\n",
        "    # Prefer the most original text available\n",
        "    base = df['text_clean'] if 'text_clean' in df.columns else df['text']\n",
        "    df['text_with_stopwords'] = base.astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfjrVyKWvXN1"
      },
      "source": [
        "## Stop Word Removal (Sanskriti Khadka)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUmi3nKb1m4E"
      },
      "outputs": [],
      "source": [
        "import re, nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load stops\n",
        "try:\n",
        "    _ = stopwords.words(\"english\")\n",
        "except LookupError:\n",
        "    nltk.download(\"stopwords\")\n",
        "\n",
        "# base list\n",
        "BASE = set(stopwords.words(\"english\"))\n",
        "# keep negations\n",
        "KEEP = {\"no\",\"not\",\"nor\",\"n't\",\"without\",\"against\"}\n",
        "BASE = {w for w in BASE if w not in KEEP}\n",
        "# filler words\n",
        "FILL = {\"said\",\"says\",\"say\",\"mr\",\"ms\",\"mrs\"}\n",
        "\n",
        "# tokenizer\n",
        "_tok = re.compile(r\"[A-Za-z]+\")\n",
        "\n",
        "# remove stops\n",
        "def rm_stops(s):\n",
        "    toks = _tok.findall(str(s).lower())\n",
        "    drop = BASE | FILL\n",
        "    return \" \".join(t for t in toks if t not in drop and len(t) > 1)\n",
        "\n",
        "# add raw\n",
        "for df in (true_df_cleaned, fake_df_cleaned):\n",
        "    if \"text_raw\" not in df.columns:\n",
        "        base = df[\"text_clean\"] if \"text_clean\" in df.columns else df[\"text\"]\n",
        "        df[\"text_raw\"] = base.astype(str)\n",
        "\n",
        "# write to text\n",
        "for df in (true_df_cleaned, fake_df_cleaned):\n",
        "    df[\"text\"] = df[\"text_raw\"].apply(rm_stops)\n",
        "\n",
        "# stats lines (your format)\n",
        "true_before = true_df_cleaned['text_raw'].str.split().str.len().mean()\n",
        "true_after  = true_df_cleaned['text'].str.split().str.len().mean()\n",
        "\n",
        "fake_before = fake_df_cleaned['text_raw'].str.split().str.len().mean()\n",
        "fake_after  = fake_df_cleaned['text'].str.split().str.len().mean()\n",
        "\n",
        "print(\"True avg:\", round(true_before,1), \"→\", round(true_after,1))\n",
        "print(\"Fake avg:\", round(fake_before,1), \"→\", round(fake_after,1))\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = [\"True Before\", \"True After\", \"Fake Before\", \"Fake After\"]\n",
        "values = [true_before, true_after, fake_before, fake_after]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, values)\n",
        "plt.title(\"Average Word Count Before vs After Stopword Removal\")\n",
        "plt.ylabel(\"Average Word Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860O49gvvgz3"
      },
      "source": [
        "## Removal of Non-English Text (Kashvi Vijay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0_ZRuvJPgq6"
      },
      "outputs": [],
      "source": [
        "# Removal of non-English text\n",
        "# Filter out non-English rows\n",
        "# true_df = true_df[true_df['is_english']]\n",
        "# fake_df = fake_df[fake_df['is_english']]\n",
        "true_df_cleaned = true_df_cleaned[true_df_cleaned[\"is_english\"]]\n",
        "fake_df_cleaned = fake_df_cleaned[fake_df_cleaned[\"is_english\"]]\n",
        "print(\"True articles remaining:\", len(true_df_cleaned))\n",
        "print(\"Fake articles remaining:\", len(fake_df_cleaned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4MGKwWvqLB"
      },
      "source": [
        "## Date Standardization (Kashvi Vijay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHV8oH2gFgSM"
      },
      "outputs": [],
      "source": [
        "#Kashvi - final date standardization\n",
        "# Convert to datetime objects first, coercing errors\n",
        "true_df_cleaned['date_clean'] = pd.to_datetime(true_df['date'], format='mixed', errors='coerce')\n",
        "fake_df_cleaned['date_clean'] = pd.to_datetime(fake_df['date'], format='mixed', errors='coerce')\n",
        "\n",
        "# Convert valid datetime objects to 'MM-DD-YYYY' string format\n",
        "# Keep original date string if conversion failed\n",
        "true_df_cleaned['date'] = true_df['date_clean'].dt.strftime('%m-%d-%Y').fillna(true_df['date'])\n",
        "fake_df_cleaned['date'] = fake_df['date_clean'].dt.strftime('%m-%d-%Y').fillna(fake_df['date'])\n",
        "\n",
        "print(\"Date standardization to MM-DD-YYYY complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOdzS-CIvxoz"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOjlFjmjm_or"
      },
      "outputs": [],
      "source": [
        "true_df_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lObEokMnFKL"
      },
      "outputs": [],
      "source": [
        "fake_df_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1yAdnRnJqlR"
      },
      "source": [
        "## One Hot Encoding (Lin Zhang)\n",
        "Why This Matters:\n",
        "- One-hot encoding converts categorical data into binary features (0/1) that models can process\n",
        "- Text length is a meaningful feature because fake news articles often have different lengths than real news\n",
        "\n",
        "What It Means:\n",
        "- Each article gets 4 binary columns: `text_length_category_short`, `text_length_category_medium`, `text_length_category_long`, `text_length_category_empty`\n",
        "- Only ONE column is 1 (the category the article belongs to), others are 0\n",
        "\n",
        "Why We Suggest This:\n",
        "- Easy to understand which length category affects predictions\n",
        "- No loss of information compared to label encoding\n",
        "- Common practice for categorical ML features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3wcGSyTKCxF"
      },
      "outputs": [],
      "source": [
        "print(f\"True dataset shape encoding: {true_df_cleaned.shape}\")\n",
        "print(f\"Fake dataset shape encoding: {fake_df_cleaned.shape}\")\n",
        "def categorize_text_length(text):\n",
        "    \"\"\"Categorize text into length categories\"\"\"\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return \"empty\"\n",
        "    word_count = len(str(text).split())\n",
        "    if word_count < 100:\n",
        "        return \"short\"\n",
        "    elif word_count < 500:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"long\"\n",
        "\n",
        "true_df_cleaned['text_length_category'] = true_df_cleaned['text'].apply(categorize_text_length)\n",
        "fake_df_cleaned['text_length_category'] = fake_df_cleaned['text'].apply(categorize_text_length)\n",
        "\n",
        "def perform_one_hot_encoding(df, categorical_columns):\n",
        "    \"\"\"Perform one-hot encoding on specified categorical columns\"\"\"\n",
        "    df_encoded = df.copy()\n",
        "    for col in categorical_columns:\n",
        "        if col in df_encoded.columns:\n",
        "            unique_values = df_encoded[col].unique()\n",
        "            # Create dummy variables (one-hot encoding) as 0/1 integers\n",
        "            dummies = pd.get_dummies(df_encoded[col], prefix=col, dtype=np.uint8)\n",
        "            df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
        "    return df_encoded\n",
        "\n",
        "true_df_encoded = perform_one_hot_encoding(true_df_cleaned, ['text_length_category'])\n",
        "fake_df_encoded = perform_one_hot_encoding(fake_df_cleaned, ['text_length_category'])\n",
        "print(\"Available columns for one-hot encoding:\")\n",
        "print(\"True dataset columns:\", list(true_df_cleaned.columns))\n",
        "print(\"Fake dataset columns:\", list(fake_df_cleaned.columns))\n",
        "categorical_features = []\n",
        "\n",
        "true_df_cleaned['text_length_category'] = true_df_cleaned['text'].apply(categorize_text_length)\n",
        "fake_df_cleaned['text_length_category'] = fake_df_cleaned['text'].apply(categorize_text_length)\n",
        "print(\"Text length categories in True dataset:\")\n",
        "print(true_df_cleaned['text_length_category'].value_counts())\n",
        "\n",
        "print(\"\\nText length categories in Fake dataset:\")\n",
        "print(fake_df_cleaned['text_length_category'].value_counts())\n",
        "\n",
        "#Perform One Hot Encoding\n",
        "true_df_encoded = perform_one_hot_encoding(true_df_cleaned, ['text_length_category'])\n",
        "fake_df_encoded = perform_one_hot_encoding(fake_df_cleaned, ['text_length_category'])\n",
        "print(\"\\nTrue dataset after one-hot encoding:\")\n",
        "print(\"New columns:\", [col for col in true_df_encoded.columns if 'text_length_category' in col])\n",
        "\n",
        "print(\"\\nFake dataset after one-hot encoding:\")\n",
        "print(\"New columns:\", [col for col in fake_df_encoded.columns if 'text_length_category' in col])\n",
        "\n",
        "print(\"\\nSample of encoded True data:\")\n",
        "print(true_df_encoded[['text_length_category'] + [col for col in true_df_encoded.columns if 'text_length_category_' in col]].head())\n",
        "\n",
        "print(\"\\nSample of encoded Fake data:\")\n",
        "print(fake_df_encoded[['text_length_category'] + [col for col in fake_df_encoded.columns if 'text_length_category_' in col]].head())\n",
        "\n",
        "true_df_cleaned = true_df_encoded\n",
        "fake_df_cleaned = fake_df_encoded\n",
        "\n",
        "print(f\"True dataset shape after encoding: {true_df_cleaned.shape}\")\n",
        "print(f\"Fake dataset shape after encoding: {fake_df_cleaned.shape}\")\n",
        "print(f\"True dataset columns after encoding: {true_df_cleaned.columns}\")\n",
        "print(f\"Fake dataset columns after encoding: {fake_df_cleaned.columns}\")\n",
        "\n",
        "\n",
        "true_df_cleaned.head()\n",
        "\n",
        "fake_df_cleaned.head()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtAAMVMbKXr2"
      },
      "outputs": [],
      "source": [
        "# # Get encoded columns\n",
        "# encoded_cols = [col for col in true_df_cleaned.columns if 'text_length_category_' in col]\n",
        "\n",
        "# if encoded_cols:\n",
        "#     # Create the heatmap\n",
        "#     plt.figure(figsize=(12, 8))\n",
        "\n",
        "#     # Sample first 100 rows for visualization\n",
        "#     encoded_matrix = true_df_cleaned[encoded_cols].head(100).values\n",
        "\n",
        "#     # Create heatmap\n",
        "#     im = plt.imshow(encoded_matrix.T, cmap='RdYlBu', aspect='auto')\n",
        "\n",
        "#     plt.title('Text Length One-Hot Encoding Matrix\\n(Each row = 1, others = 0)', fontsize=14, fontweight='bold')\n",
        "#     plt.xlabel('Data Points (First 100)', fontsize=12)\n",
        "#     plt.ylabel('Encoded Categories', fontsize=12)\n",
        "\n",
        "#     plt.yticks(range(len(encoded_cols)),\n",
        "#                [col.replace('text_length_category_', '') for col in encoded_cols])\n",
        "\n",
        "#     plt.colorbar(im, label='Value (0 or 1)')\n",
        "\n",
        "#     plt.grid(True, alpha=0.3)\n",
        "\n",
        "#     # Save and show\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe6oc3eurMt4"
      },
      "source": [
        "## Word N-gram (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UoFO0qRqP2i"
      },
      "source": [
        "Notes: Using n-gram, we can vectorize with TF-IDF or counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfJsdJQMKDJD"
      },
      "outputs": [],
      "source": [
        "# column indicates which column n-gram is done on\n",
        "# n determines whether it is a uni, bi, tri and so on\n",
        "# top_k indicate how much to return\n",
        "def word_ngrams(df, col, n=1, top_k=20, min_df=1):\n",
        "    texts = df[col].astype(str).tolist()\n",
        "    vec = CountVectorizer(ngram_range=(n, n), lowercase=True, token_pattern=r\"(?u)\\b[\\w-]+\\b\", min_df=min_df)\n",
        "    X = vec.fit_transform(texts)\n",
        "    counts = np.asarray(X.sum(axis=0)).ravel()\n",
        "    vocab = vec.get_feature_names_out()\n",
        "    out = (pd.DataFrame({\"ngram\": vocab, \"count\": counts}).sort_values(\"count\", ascending=False).head(top_k).reset_index(drop=True))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InSMZcA7Juno"
      },
      "source": [
        "### Word n-gram for Text (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbdxrhKxnW_C"
      },
      "outputs": [],
      "source": [
        "# Top 20 unigrams/bigrams/trigrams in TRUE articles\n",
        "top_uni_true  = word_ngrams(true_df_cleaned, col=\"text\", n=1, top_k=20)\n",
        "top_bi_true   = word_ngrams(true_df_cleaned, col=\"text\", n=2, top_k=20)\n",
        "top_tri_true  = word_ngrams(true_df_cleaned, col=\"text\", n=3, top_k=20)\n",
        "\n",
        "# Top 20 unigrams/bigrams/trigrams in FAKE articles\n",
        "top_uni_fake  = word_ngrams(fake_df_cleaned, col=\"text\", n=1, top_k=20)\n",
        "top_bi_fake   = word_ngrams(fake_df_cleaned, col=\"text\", n=2, top_k=20)\n",
        "top_tri_fake  = word_ngrams(fake_df_cleaned, col=\"text\", n=3, top_k=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnf9O71Dw8Dg"
      },
      "outputs": [],
      "source": [
        "# Show\n",
        "display(top_uni_true)\n",
        "display(top_uni_fake)\n",
        "display(top_bi_true)\n",
        "display(top_bi_fake)\n",
        "display(top_tri_true)\n",
        "display(top_tri_fake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-9AaJIbJ0_e"
      },
      "source": [
        "### Word n-gram for Title (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkYJEuYlKDlu"
      },
      "outputs": [],
      "source": [
        "# Top n-grams on TITLE (True vs Fake)\n",
        "top_uni_title_true = word_ngrams(true_df_cleaned, col=\"title\", n=1, top_k=20, min_df=2)\n",
        "top_bi_title_true  = word_ngrams(true_df_cleaned, col=\"title\", n=2, top_k=20, min_df=2)\n",
        "top_tri_title_true = word_ngrams(true_df_cleaned, col=\"title\", n=3, top_k=20, min_df=2)\n",
        "\n",
        "top_uni_title_fake = word_ngrams(fake_df_cleaned, col=\"title\", n=1, top_k=20, min_df=2)\n",
        "top_bi_title_fake  = word_ngrams(fake_df_cleaned, col=\"title\", n=2, top_k=20, min_df=2)\n",
        "top_tri_title_fake = word_ngrams(fake_df_cleaned, col=\"title\", n=3, top_k=20, min_df=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DcD7CuDxBRK"
      },
      "outputs": [],
      "source": [
        "display(top_uni_title_true)\n",
        "display(top_uni_title_fake)\n",
        "display(top_bi_title_true)\n",
        "display(top_bi_title_fake)\n",
        "display(top_tri_title_true)\n",
        "display(top_tri_title_fake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uerbIHkxshg-"
      },
      "source": [
        "## Character N-gram (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK-aIeESsrAg"
      },
      "outputs": [],
      "source": [
        "def char_ngrams(df, col, ngram_range=(3,5), top_k=20, min_df=2, max_df=0.95, analyzer=\"char_wb\"):\n",
        "    texts = df[col].astype(str).tolist()\n",
        "    vec = CountVectorizer(analyzer=analyzer, ngram_range=ngram_range, lowercase=True, min_df=min_df, max_df=max_df)\n",
        "    X = vec.fit_transform(texts)\n",
        "    counts = np.asarray(X.sum(axis=0)).ravel()\n",
        "    vocab = vec.get_feature_names_out()\n",
        "    out = (pd.DataFrame({\"ngram\": vocab, \"count\": counts}).sort_values(\"count\", ascending=False).head(top_k).reset_index(drop=True))\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6gyNlAEJ4QX"
      },
      "source": [
        "### Character n-gram for Text (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-6O0cFNKEGe"
      },
      "outputs": [],
      "source": [
        "top_char_text_true = char_ngrams(true_df_cleaned, col=\"text\", ngram_range=(3,5), top_k=20)\n",
        "top_char_text_fake = char_ngrams(fake_df_cleaned,  col=\"text\", ngram_range=(3,5), top_k=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyViSmR-xF5v"
      },
      "outputs": [],
      "source": [
        "display(top_char_text_true)\n",
        "display(top_char_text_fake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrjT0Jp3J7Gl"
      },
      "source": [
        "### Character n-gram for Title (Adriena Jiang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14I96qqXm312"
      },
      "outputs": [],
      "source": [
        "top_char_title_true = char_ngrams(true_df_cleaned, col=\"title\", ngram_range=(3,5), top_k=20)\n",
        "top_char_title_fake = char_ngrams(fake_df_cleaned,  col=\"title\", ngram_range=(3,5), top_k=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u0wIzbuxGWI"
      },
      "outputs": [],
      "source": [
        "display(top_char_title_true)\n",
        "display(top_char_title_fake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOSxcywj7E9g"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add dataset labels\n",
        "true_text = top_char_text_true.copy()\n",
        "fake_text = top_char_text_fake.copy()\n",
        "\n",
        "true_text[\"Dataset\"] = \"True\"\n",
        "fake_text[\"Dataset\"] = \"Fake\"\n",
        "\n",
        "compare_text = pd.concat([true_text, fake_text])\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "sns.barplot(data=compare_text, x=\"count\", y=\"ngram\", hue=\"Dataset\")\n",
        "plt.title(\"Top Character N-grams in TEXT: True vs Fake\", fontsize=15)\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Character N-gram\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Add dataset labels\n",
        "true_title = top_char_title_true.copy()\n",
        "fake_title = top_char_title_fake.copy()\n",
        "\n",
        "true_title[\"Dataset\"] = \"True\"\n",
        "fake_title[\"Dataset\"] = \"Fake\"\n",
        "\n",
        "compare_title = pd.concat([true_title, fake_title])\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "sns.barplot(data=compare_title, x=\"count\", y=\"ngram\", hue=\"Dataset\")\n",
        "plt.title(\"Top Character N-grams in TITLES: True vs Fake\", fontsize=15)\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Character N-gram\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXGrTRuGbeMa"
      },
      "source": [
        "## Sentiment Features (Sanskriti Khadka)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuNboJjMbZSe"
      },
      "outputs": [],
      "source": [
        "# Sentiment Features - Sanskriti Khadka\n",
        "# takes 2 mins to run\n",
        "\n",
        "TEXT_COL = 'text'\n",
        "\n",
        "# Initialize VADER analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to extract sentiment scores for each article\n",
        "def get_vader_scores(text):\n",
        "    scores = analyzer.polarity_scores(str(text))\n",
        "    return pd.Series({\n",
        "        'vader_neg': scores['neg'],\n",
        "        'vader_neu': scores['neu'],\n",
        "        'vader_pos': scores['pos'],\n",
        "        'vader_compound': scores['compound']\n",
        "    })\n",
        "\n",
        "# Applying to both datasets\n",
        "true_df_cleaned[['vader_neg','vader_neu','vader_pos','vader_compound']] = true_df_cleaned[TEXT_COL].apply(get_vader_scores)\n",
        "fake_df_cleaned[['vader_neg','vader_neu','vader_pos','vader_compound']] = fake_df_cleaned[TEXT_COL].apply(get_vader_scores)\n",
        "\n",
        "# Sample results\n",
        "print(f\"\\nSample TRUE news sentiment scores:\")\n",
        "display(true_df_cleaned[['text', 'vader_neg','vader_neu','vader_pos','vader_compound']].head())\n",
        "\n",
        "print(f\"\\nSample FAKE news sentiment scores:\")\n",
        "display(fake_df_cleaned[['text', 'vader_neg','vader_neu','vader_pos','vader_compound']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXnPndYR8uST"
      },
      "outputs": [],
      "source": [
        "#Visualize the sentiment analysis\n",
        "sent_cols = [\"vader_neg\", \"vader_neu\", \"vader_pos\", \"vader_compound\"]\n",
        "\n",
        "avg_true = true_df_cleaned[sent_cols].mean()\n",
        "avg_fake = fake_df_cleaned[sent_cols].mean()\n",
        "\n",
        "avg_df = pd.DataFrame({\n",
        "    \"sentiment\": sent_cols,\n",
        "    \"True\": avg_true.values,\n",
        "    \"Fake\": avg_fake.values\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "avg_df.set_index(\"sentiment\").plot(kind=\"bar\", figsize=(10,5), color=[\"blue\",\"red\"])\n",
        "plt.title(\"Average Sentiment Scores – TRUE vs FAKE\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xehu1GKh7ymZ"
      },
      "source": [
        "## Train Test Split (Kashvi Vijay)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2476fdd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine the true and fake dataframes and add a 'label' column\n",
        "true_df_cleaned['label'] = 0  # 0 for true\n",
        "fake_df_cleaned['label'] = 1  # 1 for fake\n",
        "\n",
        "combined_df = pd.concat([true_df_cleaned, fake_df_cleaned], ignore_index=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Using 'text' column as feature for now, other features can be added later\n",
        "X = combined_df['text']\n",
        "y = combined_df['label']\n",
        "\n",
        "# train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Train set shape (X_train):\", X_train.shape)\n",
        "print(\"Test set shape (X_test):\", X_test.shape)\n",
        "print(\"Train set shape (y_train):\", y_train.shape)\n",
        "print(\"Test set shape (y_test):\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42F7SAyf81yf"
      },
      "outputs": [],
      "source": [
        "#Visulaize Test/Split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sizes = [len(X_train), len(X_test)]\n",
        "labels = ['Training Set (80%)', 'Test Set (20%)']\n",
        "colors = ['#5A8DEE', '#FF6F61']\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "plt.title(\"Train–Test Split (80/20)\", fontsize=14, fontweight='bold')\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(['Train', 'Test'], [len(X_train), len(X_test)], color=['#5A8DEE','#FF6F61'])\n",
        "plt.title(\"Train–Test Split (80/20)\", fontsize=14, fontweight='bold')\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOSh_3pNDgbN"
      },
      "source": [
        "## TF-IDF (Nancy Huang)\n",
        "\n",
        "converts raw text data into numerical feature representation using TF-IDF features generates word-level and character-level. The two matrices returned will be combined to used to train the classification models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UU5bE6gDuhq"
      },
      "outputs": [],
      "source": [
        "# initialize vectorizer to look at words and common phrases (unigrams + bigrams)\n",
        "word_tfidf = TfidfVectorizer(\n",
        "    analyzer=\"word\",\n",
        "    ngram_range=(1,2),\n",
        "    min_df=5,\n",
        "    max_df=0.95,\n",
        "    max_features=10000,\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "# initialize vectorizer to look at character-level n-grams (3-5 characters)\n",
        "char_tfidf= TfidfVectorizer(\n",
        "    analyzer=\"char\",\n",
        "    ngram_range=(3,5),\n",
        "    min_df=5,\n",
        "    max_features=10000\n",
        ")\n",
        "#fits the vectorizers on the training text\n",
        "#learns the vocabulary and computes the TF-IDF weight\n",
        "X_train_word_tfidf = word_tfidf.fit_transform(X_train)\n",
        "X_train_char_tfidf = char_tfidf.fit_transform(X_train)\n",
        "\n",
        "#apply the learned word vocabular to the test data\n",
        "X_test_word = word_tfidf.transform(X_test)\n",
        "X_test_char = char_tfidf.transform(X_test)\n",
        "\n",
        "#combining the word and character TF-IDF matrices to feature both types\n",
        "X_train_tfidf = hstack([X_train_word_tfidf, X_train_char_tfidf]).tocsr()\n",
        "X_test_tfidf  = hstack([X_test_word, X_test_char]).tocsr()\n",
        "\n",
        "#rows correspond to the number of articles\n",
        "#columns corresponds to the number of features (vocabulary size)\n",
        "print(\"TF-IDF shapes:\")\n",
        "print(\"  X_train_word :\", X_train_word_tfidf.shape)\n",
        "print(\"  X_train_char :\", X_train_char_tfidf.shape)\n",
        "print(\"  X_train_tfidf:\", X_train_tfidf.shape)\n",
        "print(\"  X_test_word  :\", X_test_word.shape)\n",
        "print(\"  X_test_tfidf :\", X_test_tfidf.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkhbgRKQ9Ipf"
      },
      "outputs": [],
      "source": [
        "#Visualize TF-IDF\n",
        "X_train_word_tfidf.shape   # (rows, word features)\n",
        "X_train_char_tfidf.shape   # (rows, char features)\n",
        "X_train_tfidf.shape        # combined\n",
        "\n",
        "\n",
        "tfidf_sizes = pd.DataFrame({\n",
        "    \"Type\": [\"Word TF-IDF\", \"Char TF-IDF\", \"Combined TF-IDF\"],\n",
        "    \"Features\": [\n",
        "        X_train_word_tfidf.shape[1],\n",
        "        X_train_char_tfidf.shape[1],\n",
        "        X_train_tfidf.shape[1]\n",
        "    ]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=tfidf_sizes, x=\"Type\", y=\"Features\", palette=\"Purples\")\n",
        "plt.title(\"TF-IDF Feature Space Size\")\n",
        "plt.ylabel(\"Number of Features\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "\n",
        "sizes = [\n",
        "    X_train_word_tfidf.shape[1],\n",
        "    X_train_char_tfidf.shape[1]\n",
        "]\n",
        "\n",
        "plt.bar([\"Combined\"], [sum(sizes)], color=\"lightgray\", edgecolor=\"black\")\n",
        "plt.bar([\"Combined\"], [sizes[0]], color=\"blue\", label=\"Word TF-IDF\")\n",
        "plt.bar([\"Combined\"], [sizes[1]], bottom=[sizes[0]], color=\"purple\", label=\"Char TF-IDF\")\n",
        "\n",
        "plt.title(\"Composition of Combined TF-IDF Features\")\n",
        "plt.ylabel(\"Number of Features\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz1lIid9v0vA"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-x7ZvyLAzcx"
      },
      "source": [
        "## Logistic Regression (Kashvi Vijay)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5mrhox6ZcaT"
      },
      "source": [
        "**Kashvi Vijay - Model Optimizations**\n",
        "\n",
        "\n",
        "Orgininally, I tried using GridSearchCV, but noticed that it was inefficient, so I switched my approach. This approach is more efficient than traditional GridSearchCV because HalvingGridSearchCV adaptively focuses on the most promising hyperparameters instead of testing every single combination. By caching TF-IDF transformations and using fewer cross-validation folds, the tuning process runs much faster while maintaining similar accuracy. Overall, it’s a quicker and more practical way to optimize models for text-based machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec86aea2"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from joblib import Memory\n",
        "import tempfile\n",
        "import numpy as np\n",
        "\n",
        "# -------------------- sampling --------------------\n",
        "# sample ~10% of the data for tuning (e.g., 4k if 40k total)\n",
        "rng = np.random.default_rng(42)\n",
        "sample_size = int(0.1 * len(X_train))\n",
        "idx = rng.choice(len(X_train), sample_size, replace=False)\n",
        "\n",
        "# robust indexing (works for pandas, numpy, or list)\n",
        "def subset(data, indices):\n",
        "    if hasattr(data, \"iloc\"):      # pandas DataFrame or Series\n",
        "        return data.iloc[indices]\n",
        "    elif hasattr(data, \"__getitem__\"):  # list or np.array\n",
        "        return [data[i] for i in indices]\n",
        "    else:\n",
        "        raise TypeError(\"Unsupported data type for X_train/y_train\")\n",
        "\n",
        "X_sub = subset(X_train, idx)\n",
        "y_sub = subset(y_train, idx)\n",
        "\n",
        "# -------------------- cache --------------------\n",
        "cachedir = tempfile.mkdtemp()\n",
        "memory = Memory(cachedir, verbose=0)\n",
        "\n",
        "# -------------------- tf-idf --------------------\n",
        "char_tfidf.set_params(max_features=3000)\n",
        "\n",
        "# -------------------- pipeline --------------------\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', char_tfidf),\n",
        "    ('logreg', LogisticRegression(\n",
        "        solver='saga',\n",
        "        max_iter=80,\n",
        "        tol=5e-3,\n",
        "        random_state=42\n",
        "    ))\n",
        "], memory=memory)\n",
        "\n",
        "# -------------------- lean grid --------------------\n",
        "param_grid = {\n",
        "    'logreg__C': [0.5, 1.0],\n",
        "    'logreg__penalty': ['l2']\n",
        "}\n",
        "\n",
        "# -------------------- halving search --------------------\n",
        "grid_search = HalvingGridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=2,\n",
        "    factor=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=0,\n",
        "    aggressive_elimination=True\n",
        ")\n",
        "\n",
        "print(f\"Running quick halving grid search on {sample_size:,} samples…\")\n",
        "grid_search.fit(X_sub, y_sub)\n",
        "\n",
        "# -------------------- evaluate on full test --------------------\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nBest params:\", grid_search.best_params_)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fde55826"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# # grid search gives the best model\n",
        "# best_model = grid_search.best_estimator_\n",
        "\n",
        "# # predictions on test\n",
        "# y_pred = best_model.predict(X_test)\n",
        "\n",
        "# # evaluate model\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# report = classification_report(y_test, y_pred)\n",
        "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "# print(\"\\nClassification Report:\")\n",
        "# print(report)\n",
        "# print(\"\\nConfusion Matrix:\")\n",
        "# print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xHMv_R_-v8"
      },
      "source": [
        "## BERT (Nancy Huang, Lin Zhang, Adriena Jiang)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lst_-u6pzlFY"
      },
      "source": [
        "RESULTS TOO SUSPICIOUS, need to re-evaluate data cleaning, data leakage may (definietly) have occured\n",
        "\n",
        "Discussion topics during meeting: we have to go back to data cleaning and resolve some issues that causing our data leak and aspects we did not fully consider before.\n",
        "\n",
        "To start --> I decided to remove all URL tokens from the articles because after thinking about it, having \"< URL >\" is also giving away that fake news are fake because it shown that only fake news articles really had those links so it was reasonable to drop them completely.\n",
        "\n",
        "Some additional patterns I noticed:\n",
        "\n",
        "- we should drop the subject column (I remember we decided to reinstate this but there is a lot of political news which makes it easy for the model to catch up on the fake/true news)\n",
        "- removing stop words consideration, after doing some research it seems like removing stop words is hurting the BERT model's training so we should consider cloning and having a seperate dataframe dedicated to BERT (or use the one where stop words aren't removed) : text_for_bert\n",
        "- thoughts on removing: location/social media handles\n",
        "- ngrams showed that the fake news articles also had image urls and some videos which does not help us in any way so thoughts on removing them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_qqXNuukhmC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import gc\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECJBLrf25QCE"
      },
      "outputs": [],
      "source": [
        "print(\"GPU Available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz-Vd2MJogLp"
      },
      "source": [
        "BERT stands for Bidirectional Encoder Represenatation from Transformers\n",
        "\n",
        "It's a pretrained language model that understands the context of the text in both directions so left to right and right to left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BntQmD0KkFaY"
      },
      "outputs": [],
      "source": [
        "#for quick testing, will affect the results\n",
        "SAMPLE_SIZE = 5000\n",
        "print(f\"\\nUsing {SAMPLE_SIZE} training samples for faster training\")\n",
        "X_train_sample = X_train.sample(n=min(SAMPLE_SIZE, len(X_train)), random_state=42)\n",
        "y_train_sample = y_train.loc[X_train_sample.index]\n",
        "\n",
        "X_test_sample = X_test.sample(n=min(1000, len(X_test)), random_state=42)\n",
        "y_test_sample = y_test.loc[X_test_sample.index]\n",
        "\n",
        "# converts raw text into numbers so that BERT can understand them through tokenization\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True, #CLS (start) and SEP (end) tokens\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\"Loading BERT model...\")\n",
        "#loads the pretrained BERT from Google\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = NewsDataset(X_train_sample, y_train_sample, tokenizer)\n",
        "test_dataset = NewsDataset(X_test_sample, y_test_sample, tokenizer)\n",
        "\n",
        "#GPU processing (switched from CPU because it was taking way too long)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Testing batches: {len(test_loader)}\")\n",
        "\n",
        "#trying to reduce error by learning from mistakes\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "epochs = 2\n",
        "#training\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        #bidirectional\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        if batch_count % 50 == 0:\n",
        "            print(f\"Batch {batch_count}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        if batch_count % 100 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"\\nAverage Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "#extra tests and training on model on unseen data\n",
        "    print(\"Evaluating on test set...\")\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions, target_names=['True News', 'Fake News']))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(true_labels, predictions))\n",
        "\n",
        "# del model\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ6KaXDJ9u1k"
      },
      "outputs": [],
      "source": [
        "print(\"Train distribution:\")\n",
        "print(y_train_sample.value_counts())\n",
        "print(\"\\nTest distribution:\")\n",
        "print(y_test_sample.value_counts())\n",
        "\n",
        "from collections import Counter\n",
        "print(\"\\nPrediction distribution:\")\n",
        "print(Counter(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8waok7rA15ni"
      },
      "source": [
        "Model without exclamation and question marks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yLVF6lc2fxc"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    acc_baseline = accuracy_no_locations\n",
        "except NameError:\n",
        "    # If location removal test wasn't run, use the original accuracy\n",
        "    try:\n",
        "        acc_baseline = acc_with_locations\n",
        "    except NameError:\n",
        "        # If neither exists, use the original accuracy variable\n",
        "        acc_baseline = accuracy\n",
        "print(f\"\\nBaseline accuracy for punctuation test: {acc_baseline:.4f} ({acc_baseline*100:.2f}%)\")\n",
        "\n",
        "\n",
        "\"\"\"## BERT Data Leakage Analysis: Exclamation & Question Mark Removal (Lin Zhang)\n",
        "\n",
        "Testing if exclamation marks and question marks are causing data leakage.\n",
        "Analysis showed fake news has 12x more exclamation marks and question marks.\n",
        "\"\"\"\n",
        "\n",
        "def remove_exclamation_questions(text):\n",
        "    \"\"\"Remove exclamation marks and question marks from text\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return text\n",
        "\n",
        "    # Replace ! and ? with periods to maintain sentence structure\n",
        "    text_cleaned = text.replace('!', '.')\n",
        "    text_cleaned = text_cleaned.replace('?', '.')\n",
        "\n",
        "    # Clean up multiple periods\n",
        "    text_cleaned = re.sub(r'\\.{2,}', '.', text_cleaned)\n",
        "    text_cleaned = re.sub(r'\\s+', ' ', text_cleaned)\n",
        "\n",
        "    return text_cleaned.strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BERT DATA LEAKAGE ANALYSIS: Testing Exclamation & Question Mark Removal\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create version without exclamation/question marks\n",
        "print(\"\\nRemoving exclamation marks (!) and question marks (?) from text...\")\n",
        "X_train_no_punct = X_train_sample.apply(lambda x: remove_exclamation_questions(str(x)))\n",
        "X_test_no_punct = X_test_sample.apply(lambda x: remove_exclamation_questions(str(x)))\n",
        "\n",
        "print(\"Creating datasets without exclamation/question marks...\")\n",
        "# Recreate tokenizer if needed\n",
        "if 'tokenizer' not in locals() or tokenizer is None:\n",
        "    print(\"Recreating tokenizer...\")\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_dataset_no_punct = NewsDataset(X_train_no_punct, y_train_sample, tokenizer)\n",
        "test_dataset_no_punct = NewsDataset(X_test_no_punct, y_test_sample, tokenizer)\n",
        "\n",
        "# Use smaller batch size to save memory\n",
        "BATCH_SIZE_NO_PUNCT = 8\n",
        "train_loader_no_punct = DataLoader(train_dataset_no_punct, batch_size=BATCH_SIZE_NO_PUNCT, shuffle=True)\n",
        "test_loader_no_punct = DataLoader(test_dataset_no_punct, batch_size=BATCH_SIZE_NO_PUNCT, shuffle=False)\n",
        "\n",
        "print(\"Loading BERT model for punctuation-removed training...\")\n",
        "# Clear any remaining memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "model_no_punct = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model_no_punct.to(device)\n",
        "\n",
        "# Verify memory\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Model loaded. GPU memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
        "\n",
        "optimizer_no_punct = torch.optim.Adam(model_no_punct.parameters(), lr=2e-5)\n",
        "epochs = 2\n",
        "\n",
        "print(f\"\\nTraining BERT WITHOUT exclamation/question marks ({epochs} epochs)...\")\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} (NO ! ?)\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    model_no_punct.train()\n",
        "    train_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    for batch in train_loader_no_punct:\n",
        "        optimizer_no_punct.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model_no_punct(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_no_punct.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        batch_count += 1\n",
        "\n",
        "        if batch_count % 50 == 0:\n",
        "            print(f\"Batch {batch_count}/{len(train_loader_no_punct)}, Loss: {loss.item():.4f}\")\n",
        "            # Clear cache more frequently to save memory\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader_no_punct)\n",
        "    print(f\"\\nAverage Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Evaluate without exclamation/question marks\n",
        "print(\"\\nEvaluating on test set (NO ! ?)...\")\n",
        "model_no_punct.eval()\n",
        "predictions_no_punct = []\n",
        "true_labels_no_punct = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader_no_punct:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model_no_punct(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        preds = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        predictions_no_punct.extend(preds.cpu().numpy())\n",
        "        true_labels_no_punct.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy_no_punct = accuracy_score(true_labels_no_punct, predictions_no_punct)\n",
        "print(f\"Test Accuracy (NO ! ?): {accuracy_no_punct:.4f}\")\n",
        "\n",
        "# Clean up\n",
        "print(\"\\nCleaning up GPU memory...\")\n",
        "# del model_no_punct\n",
        "# del train_loader_no_punct\n",
        "# del test_loader_no_punct\n",
        "# del train_dataset_no_punct\n",
        "# del test_dataset_no_punct\n",
        "# del optimizer_no_punct\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    print(f\"Cleanup complete. Free GPU memory: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0):.2f} MB\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON: WITH vs WITHOUT Exclamation/Question Marks\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON: WITH vs WITHOUT Exclamation/Question Marks\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"┌─────────────────────────────────────────────────────────┐\")\n",
        "print(f\"│ WITH ! ? (baseline):     {acc_baseline:.4f} ({acc_baseline*100:.2f}%)                    │\")\n",
        "print(f\"│ WITHOUT ! ?:             {accuracy_no_punct:.4f} ({accuracy_no_punct*100:.2f}%)                    │\")\n",
        "print(f\"│ Difference:              {abs(acc_baseline - accuracy_no_punct):.4f} ({abs(acc_baseline - accuracy_no_punct)*100:.2f}%)                    │\")\n",
        "print(f\"└─────────────────────────────────────────────────────────┘\")\n",
        "\n",
        "print(f\"\\nAnalysis:\")\n",
        "if acc_baseline > accuracy_no_punct + 0.05:\n",
        "    print(f\"WARNING: Removing ! ? reduced accuracy by {abs(acc_baseline - accuracy_no_punct)*100:.2f}%\")\n",
        "    print(f\"   -> This suggests exclamation/question marks WERE contributing to data leakage\")\n",
        "    print(f\"   -> Model was learning '! ? = fake news' instead of content patterns\")\n",
        "    print(f\"   -> This is a legitimate signal but may be dataset-specific\")\n",
        "elif accuracy_no_punct > acc_baseline + 0.02:\n",
        "    print(f\"INFO: Removing ! ? improved accuracy by {abs(accuracy_no_punct - acc_baseline)*100:.2f}%\")\n",
        "    print(f\"   -> This suggests punctuation was adding slight noise\")\n",
        "    print(f\"   -> Use model WITHOUT punctuation\")\n",
        "else:\n",
        "    print(f\"KEY FINDING: Punctuation removal had {abs(acc_baseline - accuracy_no_punct)*100:.2f}% impact\")\n",
        "    if abs(acc_baseline - accuracy_no_punct) < 0.03:\n",
        "        print(f\"   -> Exclamation/question marks are NOT the main source of high accuracy\")\n",
        "        print(f\"   -> Other writing style features are more important\")\n",
        "    else:\n",
        "        print(f\"   -> Exclamation/question marks contribute to accuracy but not the only factor\")\n",
        "\n",
        "print(f\"\\nAccuracy Assessment:\")\n",
        "if accuracy_no_punct > 0.85:\n",
        "    print(f\"WARNING: WITHOUT ! ?: {accuracy_no_punct*100:.2f}% is still high - other leakage sources exist\")\n",
        "    print(f\"   -> Need to investigate other writing style features\")\n",
        "elif 0.65 <= accuracy_no_punct <= 0.85:\n",
        "    print(f\"SUCCESS: WITHOUT ! ?: {accuracy_no_punct*100:.2f}% is in realistic range (65-85%)\")\n",
        "    print(f\"   -> Exclamation/question marks were causing data leakage\")\n",
        "elif accuracy_no_punct < 0.65:\n",
        "    print(f\"WARNING: WITHOUT ! ?: {accuracy_no_punct*100:.2f}% is low - may need more training\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"CONCLUSION: Data Leakage Investigation\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nKey Findings:\")\n",
        "print(f\"  1. Exclamation/Question removal: {abs(acc_baseline - accuracy_no_punct)*100:.2f}% impact\")\n",
        "print(f\"  2. Baseline (with ! ?): {acc_baseline*100:.2f}%\")\n",
        "print(f\"  3. Without ! ?: {accuracy_no_punct*100:.2f}%\")\n",
        "print(f\"\\nInterpretation:\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28mnPadkeeS"
      },
      "source": [
        "\n",
        "\n",
        "# LR MODEL: Content Only (50 Features)\n",
        "\n",
        "   What We Used:\n",
        "   - Text content only (no metadata)\n",
        "   - Removes URLs, news organization names (Reuters, CNN, etc.), and social media artifacts\n",
        "   - Removes stop words (is, the, a, etc.)\n",
        "   - Converts text into numerical features using TF-IDF\n",
        "   - Top 50 most common words\n",
        "   - Strong regularization (C=0.001); Regularization reduces overfitting\n",
        "   - Cross-validation (5-fold)\n",
        "\n",
        "**Test accuracy: ~74.27%**\n",
        "\n",
        "**Cross-validation accuracy: ~72.45% ± 0.45%**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPaGWk4rjcSM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# ============================================================================\n",
        "# CROSS-VALIDATION SETUP\n",
        "# ============================================================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# ============================================================================\n",
        "# MODEL 1: Content Only - Minimal Features (50)\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL Content Only (50 Features)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n   What We Used:\")\n",
        "print(\"   - Text content only (no metadata)\")\n",
        "print(\"   - TF-IDF word features (unigrams only)\")\n",
        "print(\"   - Top 50 most common words\")\n",
        "print(\"   - Strong regularization (C=0.001)\")\n",
        "print(\"   - Cross-validation (5-fold)\")\n",
        "\n",
        "tfidf_50 = TfidfVectorizer(\n",
        "    analyzer=\"word\",\n",
        "    ngram_range=(1, 1),\n",
        "    min_df=500,\n",
        "    max_df=0.60,\n",
        "    max_features=50,\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "\n",
        "X_train_50 = tfidf_50.fit_transform(X_train)\n",
        "X_test_50 = tfidf_50.transform(X_test)\n",
        "\n",
        "lr_50 = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1, C=0.001)\n",
        "lr_50.fit(X_train_50, y_train)\n",
        "\n",
        "test_pred_50 = lr_50.predict(X_test_50)\n",
        "test_acc_50 = accuracy_score(y_test, test_pred_50)\n",
        "\n",
        "cv_scores_50 = cross_val_score(lr_50, X_train_50, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "print(f\"\\n   Results:\")\n",
        "print(f\"   Test Accuracy: {test_acc_50:.4f} ({test_acc_50*100:.2f}%)\")\n",
        "print(f\"   CV Accuracy:   {cv_scores_50.mean():.4f} ({cv_scores_50.mean()*100:.2f}%) ± {cv_scores_50.std():.4f}\")\n",
        "print(f\"   Train-Test Gap: {abs(lr_50.score(X_train_50, y_train) - test_acc_50):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYonX4nGVxnI"
      },
      "source": [
        "## Neural Networks (Sanskriti K, Ousman, Harshika)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9twl-e-cgCi"
      },
      "source": [
        "In this section, I use TensorFlow/Keras to train two neural networks that can tell apart real vs. fake news based on the text content.\n",
        "Both models take the article text, turn it into word tokens, and learn patterns directly from the words.\n",
        "\n",
        "1. AVG Model (Fast Baseline) – a simple network that averages word embeddings.\n",
        "\n",
        "\n",
        "2. CNN Model (1D Convolution) – a slightly deeper model that learns local patterns (like short word phrases) using filters.It can capture context better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK_ZBeA2dup6"
      },
      "source": [
        "Both models performed very well on the test data.\n",
        "The AVG model reached 96.3% accuracy.\n",
        "The CNN model performed even better, with 98.4% accuracy.\n",
        "The confusion matrices confirm that both models made very few mistakes, especially on real news articles.\n",
        "\n",
        "However, overfitting may exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCEJ643YWWot"
      },
      "outputs": [],
      "source": [
        "# Neural Networks with TensorFlow/Keras\n",
        "import os, gc, numpy as np, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# SAMPLING FOR FASTER TRAINING\n",
        "SAMPLE_SIZE = 5000\n",
        "\n",
        "# Sample from train/test\n",
        "X_train_sample = X_train.sample(n=min(SAMPLE_SIZE, len(X_train)), random_state=1234)\n",
        "y_train_sample = y_train.loc[X_train_sample.index]\n",
        "\n",
        "X_test_sample = X_test.sample(n=min(1000, len(X_test)), random_state=1234)\n",
        "y_test_sample = y_test.loc[X_test_sample.index]\n",
        "\n",
        "\n",
        "# DATA PREPARATION\n",
        "# Use the cleaned text column directly\n",
        "TEXT_COL_DL = 'text'  # This has stop words removed and is cleaned\n",
        "\n",
        "# get sampled texts\n",
        "train_texts = combined_df.loc[X_train_sample.index, TEXT_COL_DL].astype(str).values\n",
        "test_texts  = combined_df.loc[X_test_sample.index,  TEXT_COL_DL].astype(str).values\n",
        "y_train_np  = y_train_sample.values.astype(np.int32)\n",
        "y_test_np   = y_test_sample.values.astype(np.int32)\n",
        "\n",
        "# text -> token ids\n",
        "SEED = 1234\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "VOCAB_SIZE = 20000  # reduced from 50000 for speed\n",
        "SEQUENCE_LENGTH = 200  # reduced from 300 for speed\n",
        "\n",
        "vectorizer = layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQUENCE_LENGTH,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        ")\n",
        "vectorizer.adapt(train_texts)\n",
        "\n",
        "# fast input pipeline\n",
        "BATCH_SIZE = 128  # increased for faster training\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def make_ds(texts, labels, training=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(min(len(texts), 5000), seed=SEED)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_ds(train_texts, y_train_np, training=True)\n",
        "test_ds  = make_ds(test_texts,  y_test_np, training=False)\n",
        "\n",
        "def vec_map(x, y):\n",
        "    return vectorizer(x), y\n",
        "\n",
        "train_ds = train_ds.map(vec_map, num_parallel_calls=AUTOTUNE)\n",
        "test_ds  = test_ds.map(vec_map,  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# training helpers\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-5, verbose=1)\n",
        "\n",
        "# Model A: Embedding + GlobalAverage\n",
        "def build_avg_model(vocab_size=VOCAB_SIZE, seq_len=SEQUENCE_LENGTH, emb_dim=64, dropout=0.3):\n",
        "    inputs = layers.Input(shape=(seq_len,), dtype=tf.int64)\n",
        "    x = layers.Embedding(vocab_size, emb_dim)(inputs)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"avg_pool_model\")\n",
        "    model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "print(\"\\nTraining AVG Model\")\n",
        "print(\"-\"*50)\n",
        "avg_model = build_avg_model()\n",
        "\n",
        "history_avg = avg_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=5,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=0  # silent training\n",
        ")\n",
        "print(f\"Final Val Accuracy: {history_avg.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "# eval\n",
        "avg_probs = avg_model.predict(test_ds, verbose=0).ravel()\n",
        "avg_pred  = (avg_probs >= 0.5).astype(int)\n",
        "avg_acc   = accuracy_score(y_test_np, avg_pred)\n",
        "print(f\"\\n[AVG Model] Accuracy: {avg_acc:.4f}\")\n",
        "print(classification_report(y_test_np, avg_pred, target_names=['True', 'Fake']))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_np, avg_pred))\n",
        "\n",
        "\n",
        "# Model B: Embedding + 1D CNN\n",
        "def build_cnn_model(vocab_size=VOCAB_SIZE, seq_len=SEQUENCE_LENGTH,\n",
        "                    emb_dim=64, num_filters=64, kernel_size=5, dropout=0.3):\n",
        "    inputs = layers.Input(shape=(seq_len,), dtype=tf.int64)\n",
        "    x = layers.Embedding(vocab_size, emb_dim)(inputs)\n",
        "    x = layers.Conv1D(num_filters, kernel_size, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"cnn_1d_model\")\n",
        "    model.compile(optimizer=keras.optimizers.Adam(1e-3),  # increased from 2e-4\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "print(\"\\nTraining CNN Model\")\n",
        "print(\"-\"*50)\n",
        "cnn_model = build_cnn_model()\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=0  # silent training\n",
        ")\n",
        "print(f\"Final Val Accuracy: {history_cnn.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "# eval\n",
        "cnn_probs = cnn_model.predict(test_ds, verbose=0).ravel()\n",
        "cnn_pred  = (cnn_probs >= 0.5).astype(int)\n",
        "cnn_acc   = accuracy_score(y_test_np, cnn_pred)\n",
        "print(f\"\\n[CNN Model] Accuracy: {cnn_acc:.4f}\")\n",
        "print(classification_report(y_test_np, cnn_pred, target_names=['True', 'Fake']))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test_np, cnn_pred))\n",
        "\n",
        "# cleanup\n",
        "# del avg_model, cnn_model\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SAVE MODELS"
      ],
      "metadata": {
        "id": "TRM-uv3JvO_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "# --- Setup ---\n",
        "model_dir = 'Trained_models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "print(f\"Saving models to directory: {model_dir}/\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- 1. Logistic Regression Model (Scikit-learn/joblib) ---\n",
        "# Check if the model exists before saving\n",
        "try:\n",
        "    joblib.dump(log_reg, os.path.join(model_dir, 'logistic_regression_model.joblib'))\n",
        "    print(\"✅ Logistic Regression Model saved.\")\n",
        "except NameError:\n",
        "    print(\"⚠️ Logistic Regression model not found. Skipping...\")\n",
        "\n",
        "# --- 2. BERT Model (PyTorch/Hugging Face) ---\n",
        "# Note: BERT model needs to be saved differently if using transformers\n",
        "try:\n",
        "    # If using transformers library\n",
        "    model.save_pretrained(os.path.join(model_dir, 'bert_model'))\n",
        "    tokenizer.save_pretrained(os.path.join(model_dir, 'bert_model'))\n",
        "    print(\"✅ BERT Model saved.\")\n",
        "except NameError:\n",
        "    print(\"⚠️ BERT model not found. Skipping...\")\n",
        "\n",
        "# --- 3. AVG Model (Keras/TensorFlow) ---\n",
        "try:\n",
        "    avg_model.save(os.path.join(model_dir, 'avg_pool_model.keras'))\n",
        "    print(\"✅ AVG Pool Keras Model saved.\")\n",
        "except NameError:\n",
        "    print(\"⚠️ AVG Pool model not found. Skipping...\")\n",
        "\n",
        "# --- 4. CNN Model (Keras/TensorFlow) ---\n",
        "try:\n",
        "    cnn_model.save(os.path.join(model_dir, 'cnn_1d_model.keras'))\n",
        "    print(\"✅ CNN 1D Keras Model saved.\")\n",
        "except NameError:\n",
        "    print(\"⚠️ CNN model not found. Skipping...\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Model saving process completed.\")"
      ],
      "metadata": {
        "id": "eOmdLfyfxr-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}